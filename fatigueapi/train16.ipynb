{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "jAXDrqzlG0Rg",
        "outputId": "1831df93-ccbe-4c79-efdb-100bb1a5bff9"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4245902781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# TensorFlow/Keras is required for the CNN model\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: TensorFlow/Keras not found. CNN training will be skipped.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "\n",
        "\n",
        "# --- Configuration and File Paths ---\n",
        "RF_MODEL_PATH = \"rf_pipeline.pkl\"\n",
        "CNN_MODEL_PATH = \"cnn_model.h5\"\n",
        "FEATURE_DATASET_PATH = \"fatigue_features.csv\" # CSV file with all engineered features\n",
        "IMAGE_DATA_DIR = \"cnn_dataset/\" # Directory structure for CNN images (e.g., cnn_dataset/open, cnn_dataset/closed)\n",
        "\n",
        "# --- 1. Random Forest Training (Feature-Based Classification) ---\n",
        "\n",
        "def train_random_forest(data_path):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest classifier on engineered features.\n",
        "\n",
        "    Data should include: EAR, MAR, Pitch, Yaw, Roll, Brightness, Label\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Error: Feature dataset not found at {data_path}\")\n",
        "        print(\"Please ensure you have generated this CSV from your video processing.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Random Forest Training ---\")\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # 1. Prepare Data (Features + Target)\n",
        "    feature_cols = ['EAR', 'MAR', 'Pitch', 'Yaw', 'Roll', 'Brightness']\n",
        "    target_col = 'Label'\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 2. Define the Pipeline (StandardScaler for preprocessing + RF model)\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    # 3. Train the Model\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # 4. Evaluate and Save\n",
        "    y_pred = rf_pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nRandom Forest Test Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save the entire pipeline (scaler and model)\n",
        "    joblib.dump(rf_pipeline, RF_MODEL_PATH)\n",
        "    print(f\"\\n‚úÖ Random Forest Pipeline saved successfully as {RF_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- 2. CNN Training (Image-Based Classification, e.g., Eye State) ---\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines a simple CNN architecture for eye/mouth classification.\"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification (e.g., Open/Closed)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_cnn(data_dir):\n",
        "    \"\"\"Trains the CNN model using image data generators.\"\"\"\n",
        "    if not TENSORFLOW_AVAILABLE:\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: CNN image data directory not found at {data_dir}\")\n",
        "        print(\"Please ensure you have structured your images (e.g., cnn_dataset/0 and cnn_dataset/1).\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Starting CNN Image Model Training ---\")\n",
        "\n",
        "    # Image parameters\n",
        "    IMG_HEIGHT, IMG_WIDTH = 48, 48\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Data Augmentation and Preprocessing\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Load training and validation data\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Build and Train Model\n",
        "    input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "    cnn_model = create_cnn_model(input_shape)\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Save the Model\n",
        "    cnn_model.save(CNN_MODEL_PATH)\n",
        "    print(f\"\\n‚úÖ CNN Model saved successfully as {CNN_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the required CSV for feature model exists\n",
        "    # Create a dummy CSV for testing the script structure if the real one isn't ready\n",
        "    if not os.path.exists(FEATURE_DATASET_PATH):\n",
        "        print(f\"Creating a DUMMY dataset for {FEATURE_DATASET_PATH}...\")\n",
        "        dummy_data = {\n",
        "            'EAR': np.random.uniform(0.15, 0.40, 1000),\n",
        "            'MAR': np.random.uniform(0.10, 0.70, 1000),\n",
        "            'Pitch': np.random.uniform(-30, 30, 1000),\n",
        "            'Yaw': np.random.uniform(-30, 30, 1000),\n",
        "            'Roll': np.random.uniform(-20, 20, 1000),\n",
        "            'Brightness': np.random.uniform(0.3, 1.0, 1000),\n",
        "            'Label': np.random.randint(0, 2, 1000)\n",
        "        }\n",
        "        df_dummy = pd.DataFrame(dummy_data)\n",
        "        # Adjust labels to simulate fatigue: low EAR or high MAR/Pitch is fatigue (Label=1)\n",
        "        df_dummy.loc[(df_dummy['EAR'] < 0.20) | (df_dummy['MAR'] > 0.50) | (abs(df_dummy['Pitch']) > 20), 'Label'] = 1\n",
        "        df_dummy.to_csv(FEATURE_DATASET_PATH, index=False)\n",
        "        print(\"NOTE: Training will run on DUMMY DATA. Replace with real data for production models.\")\n",
        "\n",
        "    # 1. Train the Random Forest (RF) model\n",
        "    train_random_forest(FEATURE_DATASET_PATH)\n",
        "\n",
        "    # 2. Train the CNN model (if TensorFlow is available)\n",
        "    if TENSORFLOW_AVAILABLE:\n",
        "        # NOTE: You must prepare your image data in the IMAGE_DATA_DIR folder\n",
        "        # (e.g., cnn_dataset/0 for 'alert' images, cnn_dataset/1 for 'fatigue' images)\n",
        "        # If the directory doesn't exist, this step will be skipped with a warning.\n",
        "        train_cnn(IMAGE_DATA_DIR)\n",
        "\n",
        "    print(\"\\nTraining process complete. The generated models are ready for API integration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YkpNu-471tp",
        "outputId": "96deae35-3605-4ef0-e7a2-347d042c5e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully loaded Eye Blink Model from /content/drive/MyDrive/data_set/eye_blinks_model.pkl.\n",
            "üé• Starting prediction on video. Output saved to: output_eye_detection.mp4\n",
            "--------------------------------------------------\n",
            "‚úÖ Processing complete. Total Frames Processed: 532\n",
            "üé¨ Output video saved as output_eye_detection.mp4. You can now download it.\n",
            "Total Blinks Detected: 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# ‚ö†Ô∏è Make sure this path points to the video file you uploaded or a file accessible in your environment.\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/data_set/videoblocks-mah08252_24_bihvfqecc__c99dce9dd347502d4c4b714390d0a37d__P360.mp4\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/data_set/eye_blinks_model.pkl\"\n",
        "\n",
        "# --- Mediapipe Setup ---\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# Eye Landmark Indices for EAR calculation\n",
        "EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
        "# Fixed interpretation for prediction (adjust if prediction is inverted)\n",
        "LABEL_NAMES = {0: \"Closed\", 1: \"Open\"}\n",
        "\n",
        "# --- Feature Extraction Functions (Unchanged) ---\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    \"\"\"Calculates the Aspect Ratio (EAR) for the eye.\"\"\"\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "    ear = (vertical1 + vertical2) / (2.0 * horizontal + 1e-6)\n",
        "    return ear\n",
        "\n",
        "def extract_eye_feature(image):\n",
        "    \"\"\"Processes image to find face landmarks and calculate EAR.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb_image)\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None, None\n",
        "\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "    ear = aspect_ratio(landmarks, EYE_INDICES)\n",
        "    eye_points = [landmarks[i] for i in EYE_INDICES]\n",
        "\n",
        "    return np.array([[ear]]), eye_points\n",
        "\n",
        "# --- Main Prediction Function (Modified for Colab) ---\n",
        "\n",
        "def run_eye_blink_detection(video_path, model_path):\n",
        "    # 1. Load the Model\n",
        "    try:\n",
        "        if not os.path.exists(model_path):\n",
        "             print(f\"‚ùå ERROR: Model file not found at {model_path}. Please ensure it is uploaded.\")\n",
        "             return\n",
        "\n",
        "        eye_model = joblib.load(model_path)\n",
        "        print(f\"‚úÖ Successfully loaded Eye Blink Model from {model_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Setup Video Capture and Output Writer\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ùå ERROR: Could not open video file at {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video properties for output\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Define the output file name and video writer\n",
        "    output_filename = \"output_eye_detection.mp4\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for MP4\n",
        "    out = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    print(f\"üé• Starting prediction on video. Output saved to: {output_filename}\")\n",
        "\n",
        "    frame_count = 0\n",
        "    is_closed = False\n",
        "    blink_count = 0\n",
        "\n",
        "    # Process video frames\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # 3. Extract Feature\n",
        "        ear_feature, eye_points = extract_eye_feature(frame)\n",
        "\n",
        "        prediction_text = \"No Face Detected\"\n",
        "\n",
        "        if ear_feature is not None:\n",
        "            # 4. Predict\n",
        "            prediction = eye_model.predict(ear_feature)[0]\n",
        "\n",
        "            # 5. Interpret and Update Blink Count\n",
        "            current_state = LABEL_NAMES.get(prediction, \"Unknown\")\n",
        "            prediction_text = f\"Eye: {current_state}\"\n",
        "\n",
        "            if current_state == \"Closed\":\n",
        "                if not is_closed:\n",
        "                    is_closed = True\n",
        "            elif current_state == \"Open\" and is_closed:\n",
        "                blink_count += 1\n",
        "                is_closed = False\n",
        "\n",
        "            # 6. Visualization\n",
        "            if eye_points:\n",
        "                eye_points_np = np.array(eye_points, dtype=np.int32).reshape((-1, 1, 2))\n",
        "                cv2.polylines(frame, [eye_points_np], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "            cv2.putText(frame, f\"EAR: {ear_feature[0][0]:.2f}\", (10, 60),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "\n",
        "        # Display Prediction Text and Blink Count\n",
        "        cv2.putText(frame, prediction_text, (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, f\"Blinks: {blink_count}\", (frame.shape[1] - 180, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "        # 7. Write Frame to Output Video\n",
        "        out.write(frame)\n",
        "\n",
        "        # 8. Display one frame in the Colab output every 10 frames (optional, for monitoring)\n",
        "        # We don't use cv2.imshow() here; we use cv2_imshow from google.colab.patches\n",
        "        if frame_count % 10 == 0:\n",
        "            # You can display a small frame for monitoring, but for a full video, it's best to save it.\n",
        "            pass\n",
        "\n",
        "    # 9. Cleanup\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"‚úÖ Processing complete. Total Frames Processed: {frame_count}\")\n",
        "    print(f\"üé¨ Output video saved as {output_filename}. You can now download it.\")\n",
        "    print(f\"Total Blinks Detected: {blink_count}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Execute ---\n",
        "# Run this function to start the video processing\n",
        "run_eye_blink_detection(VIDEO_PATH, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bDDBDnz_9p_",
        "outputId": "04eed73b-52e5-4b5b-8df6-927a98399b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory already exists: /content/yawn_dataset\n",
            "Directory already exists: /content/eye_blink_dataset\n",
            "‚úÖ Datasets extraction process complete.\n",
            "\n",
            "\n",
            "==================================================\n",
            "üîπ Training Eye Blink Model...\n",
            "  -> Eye labels: Closed=0, Open=1\n",
            "‚ùå FATAL ERROR: Successfully loaded 0 samples from /content/eye_blink_dataset. Check image files or Mediapipe detection.\n",
            "‚úÖ Successfully loaded 0 samples for eye detection.\n",
            "üõë Skipping Eye Blink Model training due to insufficient data or classes.\n",
            "\n",
            "==================================================\n",
            "üîπ Training Yawn Model...\n",
            "  -> Yawn labels: No Yawn=0, Yawn=1\n",
            "  -> Processing folder: yawn (Assigned Label: 1)\n",
            "‚ùå CRITICAL WARNING: Only 1 class(es) loaded for mouth detection! Classes found: [1]\n",
            "  -> This will cause the ValueError in SVC.fit(). Check your dataset folder names ('No_Yawn', 'Yawn') or structure.\n",
            "‚úÖ Successfully loaded 185 samples for mouth detection.\n",
            "üõë Skipping Yawn Model training due to insufficient data or classes (Need at least 2 classes).\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# ----- Step 1: Extract ZIPs (Using corrected paths) -----\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "eye_zip = r\"/content/drive/MyDrive/data_set/test_eye_data.zip.zip\"\n",
        "\n",
        "# Corrected accessible paths for Colab/Linux\n",
        "extract_yawn = r\"/content/yawn_dataset\"\n",
        "extract_eye = r\"/content/eye_blink_dataset\"\n",
        "\n",
        "def safe_extract(zip_path, extract_path):\n",
        "    if os.path.exists(extract_path):\n",
        "        print(f\"Directory already exists: {extract_path}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(extract_path)):\n",
        "        os.makedirs(os.path.dirname(extract_path), exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"Extraction successful: {extract_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ERROR: Zip file not found at {zip_path}. Check your Drive path!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during extraction: {e}\")\n",
        "\n",
        "safe_extract(yawn_zip, extract_yawn)\n",
        "safe_extract(eye_zip, extract_eye)\n",
        "\n",
        "print(\"‚úÖ Datasets extraction process complete.\\n\")\n",
        "\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    \"\"\"Calculates the Aspect Ratio (similar to EAR/MAR).\"\"\"\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "    # Added a small epsilon to avoid division by zero, although highly unlikely\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal + 1e-6)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    h, w = image.shape[:2]\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "\n",
        "    if part == \"eye\":\n",
        "        # Indices for the right eye\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE)\n",
        "    else:\n",
        "        # Indices for the mouth\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH)\n",
        "    return [feature]\n",
        "\n",
        "def load_dataset(path, part=\"eye\"):\n",
        "    \"\"\"Loads images with explicit label mapping to prevent inversion and performs data integrity check.\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    # Explicitly define the labels\n",
        "    if part == \"eye\":\n",
        "        # We want closed eyes (low EAR) to be label 0, open eyes (high EAR) to be label 1.\n",
        "        LABEL_MAP = {\"closed\": 0, \"open\": 1, \"Closed\": 0, \"Open\": 1}\n",
        "        print(f\"  -> Eye labels: Closed={LABEL_MAP['Closed']}, Open={LABEL_MAP['Open']}\")\n",
        "    elif part == \"mouth\":\n",
        "        # We want non-yawn (low MAR) to be label 0, yawn (high MAR) to be label 1.\n",
        "        LABEL_MAP = {\"no_yawn\": 0, \"yawn\": 1, \"No_Yawn\": 0, \"Yawn\": 1}\n",
        "        print(f\"  -> Yawn labels: No Yawn={LABEL_MAP['no_yawn']}, Yawn={LABEL_MAP['yawn']}\")\n",
        "    else:\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    for folder_name in os.listdir(path):\n",
        "        folder_path = os.path.join(path, folder_name)\n",
        "\n",
        "        # Get label from map (handling case-insensitivity)\n",
        "        label = LABEL_MAP.get(folder_name)\n",
        "        if label is None:\n",
        "            label = LABEL_MAP.get(folder_name.lower())\n",
        "\n",
        "        if label is not None and os.path.isdir(folder_path):\n",
        "            print(f\"  -> Processing folder: {folder_name} (Assigned Label: {label})\")\n",
        "\n",
        "            for img_name in os.listdir(folder_path):\n",
        "                img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "                if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    continue\n",
        "\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    continue\n",
        "\n",
        "                feature = extract_features(img, part)\n",
        "\n",
        "                if feature:\n",
        "                    X.append(feature)\n",
        "                    y.append(label)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"‚ùå FATAL ERROR: Successfully loaded 0 samples from {path}. Check image files or Mediapipe detection.\")\n",
        "    else:\n",
        "        # üí° FIX: Check number of unique classes before returning\n",
        "        unique_classes = np.unique(y)\n",
        "        if len(unique_classes) < 2:\n",
        "            print(f\"‚ùå CRITICAL WARNING: Only {len(unique_classes)} class(es) loaded for {part} detection! Classes found: {unique_classes}\")\n",
        "            print(\"  -> This will cause the ValueError in SVC.fit(). Check your dataset folder names ('No_Yawn', 'Yawn') or structure.\")\n",
        "\n",
        "    print(f\"‚úÖ Successfully loaded {X.shape[0]} samples for {part} detection.\")\n",
        "    return X, y\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 3: Train Eye Blink Model (Fixed & Improved) -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîπ Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_dataset(extract_eye, part=\"eye\")\n",
        "\n",
        "if X_eye.shape[0] > 0 and len(np.unique(y_eye)) > 1:\n",
        "    # Use random_state for reproducible split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_eye, y_eye, test_size=0.2, random_state=42)\n",
        "\n",
        "    # IMPROVEMENT: Increase C value\n",
        "    eye_model = SVC(kernel='linear', C=10.0, random_state=42)\n",
        "    eye_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = eye_model.predict(X_test)\n",
        "    print(\"\\nEye Blink Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "    print(\"‚úÖ Saved: eye_blink_model.pkl\")\n",
        "else:\n",
        "    print(\"üõë Skipping Eye Blink Model training due to insufficient data or classes.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 4: Train Yawn Model (Improved and Protected against ValueError) -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîπ Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "\n",
        "# üí° FIX: Only attempt training if both data and multiple classes are present\n",
        "if X_yawn.shape[0] > 0 and len(np.unique(y_yawn)) > 1:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Apply C=10.0 here as well for potential accuracy boost\n",
        "    yawn_model = SVC(kernel='linear', C=10.0, random_state=42)\n",
        "    yawn_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = yawn_model.predict(X_test)\n",
        "    print(\"\\nYawn Detection Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "    print(\"‚úÖ Saved: yawn_model.pkl\")\n",
        "else:\n",
        "    print(\"üõë Skipping Yawn Model training due to insufficient data or classes (Need at least 2 classes).\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b7vjIKUB8WEj",
        "outputId": "eafaafb1-3c54-4398-bc6e-44d3b7d2aefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m959.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ee6beefd5043466e95060676792193a9",
              "pip_warning": {
                "packages": [
                  "cv2",
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_YLHJ8q3xDe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hJ0rEyNDsKV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = \"cnn_dataset\"  # Directory created by the video generator (contains 0/ and 1/)\n",
        "IMAGE_SIZE = (48, 48)     # Must match the size used in the generator script\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20               # Start with 20 epochs, you may need more\n",
        "# Renamed and updated save path to reflect the \"LFB model\" theme requested by the user\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    if not os.path.exists(data_directory):\n",
        "        print(f\"Error: Dataset directory '{data_directory}' not found.\")\n",
        "        print(\"Please run the 'video_to_cnn_dataset_generator.py' script first to generate the image data.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"Loading data from disk...\")\n",
        "\n",
        "    # Using image_dataset_from_directory for easy loading of labeled data\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42  # for reproducible shuffling\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Determine dataset size for splitting (80% train, 20% validation)\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No images found in '{data_directory}'. Check the folder structure.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}\")\n",
        "    print(f\"Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "    print(f\"Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "\n",
        "    # Prefetch data for performance\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines a simple CNN architecture suitable for small eye images.\"\"\"\n",
        "    print(\"Defining LFB CNN model architecture...\")\n",
        "    model = Sequential([\n",
        "        # First Conv/Pool block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Conv/Pool block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Conv/Pool block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification (Alert or Fatigue)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined CNN model.\"\"\"\n",
        "\n",
        "    # The input shape is (Height, Width, Color_Channels)\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"Starting LFB model training for {EPOCHS} epochs...\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "    # Load and prepare data\n",
        "    train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model, training_history = train_model(train_data, val_data)\n",
        "\n",
        "    # Save the model in the Keras format\n",
        "    trained_model.save(LFB_MODEL_SAVE_PATH) # <-- Using the new save path\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ LFB Model training completed. Model saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "    print(\"---------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhOJuDDCl6P_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Attempt to import TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Error: TensorFlow/Keras not found. Please run '!pip install tensorflow' first.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration (Set Your Paths Here) ---\n",
        "# NOTE: ‚ö†Ô∏è Change this path to your actual video file location\n",
        "DEFAULT_VIDEO_PATH = \"/content/drive/MyDrive/Video/9709794-uhd_3840_2160_25fps (1).mp4\"\n",
        "DLIB_PREDICTOR_PATH = \"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# Training Output Configuration\n",
        "DATA_DIR = \"cnn_dataset\"  # Output folder for generated images (0/ and 1/)\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "IMAGE_SIZE = (48, 48)     # Target size for CNN input\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# Feature Calculation Constants\n",
        "EAR_THRESHOLD = 0.23  # More sensitive threshold for fatigue (0.25 is too high for some videos)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "FRAME_SKIP_RATE = 5   # Process only 1 in 5 frames\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 1. DATA GENERATION FUNCTIONS (Video -> Images) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "def generate_cnn_dataset(video_path, predictor_path, output_dir):\n",
        "    \"\"\"Processes video frames to create a labeled image dataset (0/ and 1/).\"\"\"\n",
        "    print(f\"\\n--- 1.1. Starting Image Preparation from: {video_path} ---\")\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count, alert_count, fatigue_count = 0, 0, 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process a fraction of frames to speed up training and reduce correlation\n",
        "        if frame_count % FRAME_SKIP_RATE != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label: EAR below threshold is FATIGUE (1)\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic around the eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x_min = max(0, x_min); y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                resized_image = cv2.resize(cropped_image, IMAGE_SIZE)\n",
        "                final_image = resized_image # Use BGR image for CNN input (3 channels)\n",
        "\n",
        "                # Save the image\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "                image_filename = f\"frame_{frame_count:06d}_{ear_avg:.3f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert saved: {alert_count}, Fatigue saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ Image dataset preparation finished.\")\n",
        "    print(f\"Total images saved: Alert={alert_count}, Fatigue={fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    # Check if we actually generated any fatigue images\n",
        "    if fatigue_count == 0 and alert_count == 0:\n",
        "        print(\"Error: No images were generated. Check video path and dlib predictor.\")\n",
        "        return False\n",
        "    elif fatigue_count == 0:\n",
        "        print(\"Warning: Only ALERT (0) images were generated. CNN training results will be poor.\")\n",
        "        print(\"Try using a video with more eye closure/blinks, or lower the EAR_THRESHOLD.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 2. MODEL TRAINING FUNCTIONS (Images -> Model) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    print(\"\\n--- 2.1. Loading Data from Disk ---\")\n",
        "\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary', # Output is 0 or 1\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No batches found in '{data_directory}'. Cannot train LFB model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split dataset: 80% train, 20% validation\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}. Split: Train={train_size}, Validation={dataset_size - train_size}\")\n",
        "\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines the LFB CNN model architecture.\"\"\"\n",
        "    print(\"--- 2.2. Defining LFB CNN Model Architecture ---\")\n",
        "    model = Sequential([\n",
        "        # First Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_lfb_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined LFB CNN model.\"\"\"\n",
        "\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"--- 2.3. Starting LFB Model Training for {EPOCHS} epochs ---\")\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 3. MAIN EXECUTION ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ‚ö†Ô∏è Set up the arguments and path. This allows running it from command line,\n",
        "    # but defaults to the hardcoded path above.\n",
        "    parser = argparse.ArgumentParser(description=\"Full LFB Drowsiness Detection Training Pipeline.\")\n",
        "    parser.add_argument(\"--video\", type=str, default=DEFAULT_VIDEO_PATH,\n",
        "                        help=\"Path to the input video file.\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=DLIB_PREDICTOR_PATH,\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Step 1: Data Generation ---\n",
        "    success = generate_cnn_dataset(args.video, args.predictor, DATA_DIR)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\nPipeline failed at the data generation step. Please check the video and predictor paths.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Step 2: Model Training ---\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_lfb_model(train_data, val_data)\n",
        "\n",
        "        # Save the model\n",
        "        trained_model.save(LFB_MODEL_SAVE_PATH)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(f\"‚úÖ FINAL SUCCESS: LFB Model saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during model training (Step 2): {e}\")\n",
        "        print(\"Please verify the 'cnn_dataset' folder structure.\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUQdi0zFqsT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bph04sMFr2o"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Attempt to import TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Error: TensorFlow/Keras not found. Please run '!pip install tensorflow' first.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration (Set Your Paths Here) ---\n",
        "# NOTE: ‚ö†Ô∏è Change this path to your actual video file location\n",
        "DEFAULT_VIDEO_PATH = \"/content/drive/MyDrive/Video/Screen-Recording (5).mp4\"\n",
        "DLIB_PREDICTOR_PATH = \"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# Training Output Configuration\n",
        "DATA_DIR = \"cnn_dataset\"  # Output folder for generated images (0/ and 1/)\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "IMAGE_SIZE = (48, 48)     # Target size for CNN input\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# Feature Calculation Constants\n",
        "EAR_THRESHOLD = 0.23  # More sensitive threshold for fatigue (0.25 is too high for some videos)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "FRAME_SKIP_RATE = 5   # Process only 1 in 5 frames\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 1. DATA GENERATION FUNCTIONS (Video -> Images) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "def generate_cnn_dataset(video_path, predictor_path, output_dir):\n",
        "    \"\"\"Processes video frames to create a labeled image dataset (0/ and 1/).\"\"\"\n",
        "    print(f\"\\n--- 1.1. Starting Image Preparation from: {video_path} ---\")\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count, alert_count, fatigue_count = 0, 0, 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process a fraction of frames to speed up training and reduce correlation\n",
        "        if frame_count % FRAME_SKIP_RATE != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label: EAR below threshold is FATIGUE (1)\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic around the eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x_min = max(0, x_min); y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                resized_image = cv2.resize(cropped_image, IMAGE_SIZE)\n",
        "                final_image = resized_image # Use BGR image for CNN input (3 channels)\n",
        "\n",
        "                # Save the image\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "                image_filename = f\"frame_{frame_count:06d}_{ear_avg:.3f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert saved: {alert_count}, Fatigue saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ Image dataset preparation finished.\")\n",
        "    print(f\"Total images saved: Alert={alert_count}, Fatigue={fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    # Check if we actually generated any fatigue images\n",
        "    if fatigue_count == 0 and alert_count == 0:\n",
        "        print(\"Error: No images were generated. Check video path and dlib predictor.\")\n",
        "        return False\n",
        "    elif fatigue_count == 0:\n",
        "        print(\"Warning: Only ALERT (0) images were generated. CNN training results will be poor.\")\n",
        "        print(\"Try using a video with more eye closure/blinks, or lower the EAR_THRESHOLD.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 2. MODEL TRAINING FUNCTIONS (Images -> Model) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    print(\"\\n--- 2.1. Loading Data from Disk ---\")\n",
        "\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary', # Output is 0 or 1\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No batches found in '{data_directory}'. Cannot train LFB model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split dataset: 80% train, 20% validation\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}. Split: Train={train_size}, Validation={dataset_size - train_size}\")\n",
        "\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines the LFB CNN model architecture.\"\"\"\n",
        "    print(\"--- 2.2. Defining LFB CNN Model Architecture ---\")\n",
        "    model = Sequential([\n",
        "        # First Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_lfb_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined LFB CNN model.\"\"\"\n",
        "\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"--- 2.3. Starting LFB Model Training for {EPOCHS} epochs ---\")\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 3. MAIN EXECUTION ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # FIX: Remove the Jupyter/Colab kernel arguments (-f <filename>)\n",
        "    # that cause the 'unrecognized arguments' error in argparse.\n",
        "    if '-f' in sys.argv:\n",
        "        f_index = sys.argv.index('-f')\n",
        "        # Remove the '-f' flag and the following argument (the json file path)\n",
        "        sys.argv = sys.argv[:f_index] + sys.argv[f_index+2:]\n",
        "\n",
        "    # ‚ö†Ô∏è Set up the arguments and path. This allows running it from command line,\n",
        "    # but defaults to the hardcoded path above.\n",
        "    parser = argparse.ArgumentParser(description=\"Full LFB Drowsiness Detection Training Pipeline.\")\n",
        "    parser.add_argument(\"--video\", type=str, default=DEFAULT_VIDEO_PATH,\n",
        "                        help=\"Path to the input video file.\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=DLIB_PREDICTOR_PATH,\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Step 1: Data Generation ---\n",
        "    success = generate_cnn_dataset(args.video, args.predictor, DATA_DIR)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\nPipeline failed at the data generation step. Please check the video and predictor paths.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Step 2: Model Training ---\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_lfb_model(train_data, val_data)\n",
        "\n",
        "        # Save the model\n",
        "        trained_model.save(LFB_MODEL_SAVE_PATH)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(f\"‚úÖ FINAL SUCCESS: LFB Model saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during model training (Step 2): {e}\")\n",
        "        print(\"Please verify the 'cnn_dataset' folder structure.\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "HRoHE2SkHcTm",
        "outputId": "bbdc1327-a8f2-4983-923e-cb3f9d5ce608"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1718578421.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Attempt to import TensorFlow/Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \"\"\"  # noqa: E501\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_qhull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/_kdtree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Released under the scipy license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcKDTreeNode\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m __all__ = ['minkowski_distance_p', 'minkowski_distance',\n",
            "\u001b[0;32mscipy/spatial/_ckdtree.pyx\u001b[0m in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Attempt to import TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Error: TensorFlow/Keras not found. Please run '!pip install tensorflow' first.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration (Set Your Paths Here) ---\n",
        "# NOTE: ‚ö†Ô∏è Change this path to your actual video file location\n",
        "DEFAULT_VIDEO_PATH = \"/content/drive/MyDrive/Video/Screen-Recording (5).mp4\"\n",
        "DLIB_PREDICTOR_PATH = \"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# Training Output Configuration\n",
        "DATA_DIR = \"cnn_dataset\"  # Output folder for generated images (0/ and 1/)\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "LFB_MODEL_YAML_PATH = \"lfb_model_architecture.yaml\" # New YAML save path\n",
        "IMAGE_SIZE = (48, 48)     # Target size for CNN input\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# Feature Calculation Constants\n",
        "EAR_THRESHOLD = 0.23  # More sensitive threshold for fatigue (0.25 is too high for some videos)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "FRAME_SKIP_RATE = 5   # Process only 1 in 5 frames\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 1. DATA GENERATION FUNCTIONS (Video -> Images) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "def generate_cnn_dataset(video_path, predictor_path, output_dir):\n",
        "    \"\"\"Processes video frames to create a labeled image dataset (0/ and 1/).\"\"\"\n",
        "    print(f\"\\n--- 1.1. Starting Image Preparation from: {video_path} ---\")\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count, alert_count, fatigue_count = 0, 0, 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process a fraction of frames to speed up training and reduce correlation\n",
        "        if frame_count % FRAME_SKIP_RATE != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label: EAR below threshold is FATIGUE (1)\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic around the eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x_min = max(0, x_min); y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                resized_image = cv2.resize(cropped_image, IMAGE_SIZE)\n",
        "                final_image = resized_image # Use BGR image for CNN input (3 channels)\n",
        "\n",
        "                # Save the image\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "                image_filename = f\"frame_{frame_count:06d}_{ear_avg:.3f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert saved: {alert_count}, Fatigue saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ Image dataset preparation finished.\")\n",
        "    print(f\"Total images saved: Alert={alert_count}, Fatigue={fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    # Check if we actually generated any fatigue images\n",
        "    if fatigue_count == 0 and alert_count == 0:\n",
        "        print(\"Error: No images were generated. Check video path and dlib predictor.\")\n",
        "        return False\n",
        "    elif fatigue_count == 0:\n",
        "        print(\"Warning: Only ALERT (0) images were generated. CNN training results will be poor.\")\n",
        "        print(\"Try using a video with more eye closure/blinks, or lower the EAR_THRESHOLD.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 2. MODEL TRAINING FUNCTIONS (Images -> Model) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    print(\"\\n--- 2.1. Loading Data from Disk ---\")\n",
        "\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary', # Output is 0 or 1\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No batches found in '{data_directory}'. Cannot train LFB model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split dataset: 80% train, 20% validation\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}. Split: Train={train_size}, Validation={dataset_size - train_size}\")\n",
        "\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines the LFB CNN model architecture.\"\"\"\n",
        "    print(\"--- 2.2. Defining LFB CNN Model Architecture ---\")\n",
        "    model = Sequential([\n",
        "        # First Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_lfb_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined LFB CNN model.\"\"\"\n",
        "\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"--- 2.3. Starting LFB Model Training for {EPOCHS} epochs ---\")\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 3. MAIN EXECUTION ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # FIX: Remove the Jupyter/Colab kernel arguments (-f <filename>)\n",
        "    # that cause the 'unrecognized arguments' error in argparse.\n",
        "    if '-f' in sys.argv:\n",
        "        f_index = sys.argv.index('-f')\n",
        "        # Remove the '-f' flag and the following argument (the json file path)\n",
        "        sys.argv = sys.argv[:f_index] + sys.argv[f_index+2:]\n",
        "\n",
        "    # ‚ö†Ô∏è Set up the arguments and path. This allows running it from command line,\n",
        "    # but defaults to the hardcoded path above.\n",
        "    parser = argparse.ArgumentParser(description=\"Full LFB Drowsiness Detection Training Pipeline.\")\n",
        "    parser.add_argument(\"--video\", type=str, default=DEFAULT_VIDEO_PATH,\n",
        "                        help=\"Path to the input video file.\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=DLIB_PREDICTOR_PATH,\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Step 1: Data Generation ---\n",
        "    success = generate_cnn_dataset(args.video, args.predictor, DATA_DIR)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\nPipeline failed at the data generation step. Please check the video and predictor paths.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Step 2: Model Training ---\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_lfb_model(train_data, val_data)\n",
        "\n",
        "        # --- Save the model (FULL MODEL) ---\n",
        "        trained_model.save(LFB_MODEL_SAVE_PATH)\n",
        "\n",
        "        # --- Save the architecture as YAML (NEW) ---\n",
        "        # NOTE: This requires pyyaml to be installed, which is usually included in Colab/Jupyter.\n",
        "        # If running locally, you might need to install it: `pip install pyyaml`\n",
        "        yaml_model = trained_model.to_yaml()\n",
        "        with open(LFB_MODEL_YAML_PATH, \"w\") as yaml_file:\n",
        "            yaml_file.write(yaml_model)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(f\"‚úÖ FINAL SUCCESS: LFB Model (Full) saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "        print(f\"‚úÖ LFB Model (YAML Architecture) saved to: {os.path.abspath(LFB_MODEL_YAML_PATH)}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during model training (Step 2): {e}\")\n",
        "        print(\"Please verify the 'cnn_dataset' folder structure.\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "dgYYSDvRIUYm",
        "outputId": "7b750d0d-b47d-4c3e-fa4c-f704cf6cfc4e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2010395040.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Attempt to import TensorFlow/Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \"\"\"  # noqa: E501\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_qhull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/_kdtree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Released under the scipy license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcKDTreeNode\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m __all__ = ['minkowski_distance_p', 'minkowski_distance',\n",
            "\u001b[0;32mscipy/spatial/_ckdtree.pyx\u001b[0m in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Attempt to import TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    # REMOVED: model_to_yaml causes import errors in some Keras versions.\n",
        "    # We will use the more stable built-in 'trained_model.to_json()' method instead.\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Error: TensorFlow/Keras not found. Please run '!pip install tensorflow' first.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration (Set Your Paths Here) ---\n",
        "# NOTE: ‚ö†Ô∏è Path updated based on your input\n",
        "DEFAULT_VIDEO_PATH = \"/content/drive/MyDrive/Video/Screen-Recording (5).mp4\"\n",
        "DLIB_PREDICTOR_PATH = \"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# Training Output Configuration\n",
        "DATA_DIR = \"cnn_dataset\"  # Output folder for generated images (0/ and 1/)\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "# CHANGED: Switched from YAML to JSON for robust architecture serialization\n",
        "LFB_MODEL_JSON_PATH = \"lfb_model_architecture.json\"\n",
        "IMAGE_SIZE = (48, 48)     # Target size for CNN input\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# Feature Calculation Constants\n",
        "EAR_THRESHOLD = 0.23  # More sensitive threshold for fatigue (0.25 is too high for some videos)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "FRAME_SKIP_RATE = 5   # Process only 1 in 5 frames\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 1. DATA GENERATION FUNCTIONS (Video -> Images) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "def generate_cnn_dataset(video_path, predictor_path, output_dir):\n",
        "    \"\"\"Processes video frames to create a labeled image dataset (0/ and 1/).\"\"\"\n",
        "    print(f\"\\n--- 1.1. Starting Image Preparation from: {video_path} ---\")\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count, alert_count, fatigue_count = 0, 0, 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process a fraction of frames to speed up training and reduce correlation\n",
        "        if frame_count % FRAME_SKIP_RATE != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label: EAR below threshold is FATIGUE (1)\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic around the eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x_min = max(0, x_min); y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                resized_image = cv2.resize(cropped_image, IMAGE_SIZE)\n",
        "                final_image = resized_image # Use BGR image for CNN input (3 channels)\n",
        "\n",
        "                # Save the image\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "                image_filename = f\"frame_{frame_count:06d}_{ear_avg:.3f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert saved: {alert_count}, Fatigue saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ Image dataset preparation finished.\")\n",
        "    print(f\"Total images saved: Alert={alert_count}, Fatigue={fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    # Check if we actually generated any fatigue images\n",
        "    if fatigue_count == 0 and alert_count == 0:\n",
        "        print(\"Error: No images were generated. Check video path and dlib predictor.\")\n",
        "        return False\n",
        "    elif fatigue_count == 0:\n",
        "        print(\"Warning: Only ALERT (0) images were generated. CNN training results will be poor.\")\n",
        "        print(\"Try using a video with more eye closure/blinks, or lower the EAR_THRESHOLD.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 2. MODEL TRAINING FUNCTIONS (Images -> Model) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    print(\"\\n--- 2.1. Loading Data from Disk ---\")\n",
        "\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary', # Output is 0 or 1\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No batches found in '{data_directory}'. Cannot train LFB model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split dataset: 80% train, 20% validation\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}. Split: Train={train_size}, Validation={dataset_size - train_size}\")\n",
        "\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines the LFB CNN model architecture.\"\"\"\n",
        "    print(\"--- 2.2. Defining LFB CNN Model Architecture ---\")\n",
        "    model = Sequential([\n",
        "        # First Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_lfb_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined LFB CNN model.\"\"\"\n",
        "\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"--- 2.3. Starting LFB Model Training for {EPOCHS} epochs ---\")\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 3. MAIN EXECUTION ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # FIX: Remove the Jupyter/Colab kernel arguments (-f <filename>)\n",
        "    # that cause the 'unrecognized arguments' error in argparse.\n",
        "    if '-f' in sys.argv:\n",
        "        f_index = sys.argv.index('-f')\n",
        "        # Remove the '-f' flag and the following argument (the json file path)\n",
        "        sys.argv = sys.argv[:f_index] + sys.argv[f_index+2:]\n",
        "\n",
        "    # ‚ö†Ô∏è Set up the arguments and path. This allows running it from command line,\n",
        "    # but defaults to the hardcoded path above.\n",
        "    parser = argparse.ArgumentParser(description=\"Full LFB Drowsiness Detection Training Pipeline.\")\n",
        "    parser.add_argument(\"--video\", type=str, default=DEFAULT_VIDEO_PATH,\n",
        "                        help=\"Path to the input video file.\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=DLIB_PREDICTOR_PATH,\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Step 1: Data Generation ---\n",
        "    success = generate_cnn_dataset(args.video, args.predictor, DATA_DIR)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\nPipeline failed at the data generation step. Please check the video and predictor paths.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Step 2: Model Training ---\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_lfb_model(train_data, val_data)\n",
        "\n",
        "        # --- Save the model (FULL MODEL) ---\n",
        "        trained_model.save(LFB_MODEL_SAVE_PATH)\n",
        "\n",
        "        # --- Save the architecture as JSON (FIX) ---\n",
        "        # The .to_json() method is generally available on the Sequential model object.\n",
        "        json_model = trained_model.to_json()\n",
        "        with open(LFB_MODEL_JSON_PATH, \"w\") as json_file:\n",
        "            json_file.write(json_model)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(f\"‚úÖ FINAL SUCCESS: LFB Model (Full) saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "        print(f\"‚úÖ LFB Model (JSON Architecture) saved to: {os.path.abspath(LFB_MODEL_JSON_PATH)}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during model training (Step 2): {e}\")\n",
        "        print(\"Please verify the 'cnn_dataset' folder structure.\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "Fz0Jo31NKUFt",
        "outputId": "e439e1de-6d63-4825-8510-80de49f1b190"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4262574857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Attempt to import TensorFlow/Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \"\"\"  # noqa: E501\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_qhull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/spatial/_kdtree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Released under the scipy license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcKDTreeNode\u001b[0m  \u001b[0;31m# type: ignore[import-not-found]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m __all__ = ['minkowski_distance_p', 'minkowski_distance',\n",
            "\u001b[0;32mscipy/spatial/_ckdtree.pyx\u001b[0m in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# Attempt to import TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    # REMOVED: model_to_yaml causes import errors in some Keras versions.\n",
        "    # We will use the more stable built-in 'trained_model.to_json()' method instead,\n",
        "    # but save the output to a .yaml file as requested by the user.\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Error: TensorFlow/Keras not found. Please run '!pip install tensorflow' first.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- Configuration (Set Your Paths Here) ---\n",
        "# NOTE: ‚ö†Ô∏è Path updated based on your input\n",
        "DEFAULT_VIDEO_PATH = \"/content/drive/MyDrive/Video/Screen-Recording (5).mp4\"\n",
        "DLIB_PREDICTOR_PATH = \"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# Training Output Configuration\n",
        "DATA_DIR = \"cnn_dataset\"  # Output folder for generated images (0/ and 1/)\n",
        "LFB_MODEL_SAVE_PATH = \"lfb_fatigue_detector_model.keras\"\n",
        "# REVERTED TO YAML PATH, will contain JSON content for compatibility\n",
        "LFB_MODEL_YAML_PATH = \"lfb_model_architecture.yaml\"\n",
        "IMAGE_SIZE = (48, 48)     # Target size for CNN input\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "# Feature Calculation Constants\n",
        "EAR_THRESHOLD = 0.23  # More sensitive threshold for fatigue (0.25 is too high for some videos)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "FRAME_SKIP_RATE = 5   # Process only 1 in 5 frames\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 1. DATA GENERATION FUNCTIONS (Video -> Images) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "def generate_cnn_dataset(video_path, predictor_path, output_dir):\n",
        "    \"\"\"Processes video frames to create a labeled image dataset (0/ and 1/).\"\"\"\n",
        "    print(f\"\\n--- 1.1. Starting Image Preparation from: {video_path} ---\")\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}. Cannot generate dataset.\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count, alert_count, fatigue_count = 0, 0, 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process a fraction of frames to speed up training and reduce correlation\n",
        "        if frame_count % FRAME_SKIP_RATE != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label: EAR below threshold is FATIGUE (1)\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic around the eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Clamp coordinates to frame bounds\n",
        "            x_min = max(0, x_min); y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max); y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                resized_image = cv2.resize(cropped_image, IMAGE_SIZE)\n",
        "                final_image = resized_image # Use BGR image for CNN input (3 channels)\n",
        "\n",
        "                # Save the image\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "                image_filename = f\"frame_{frame_count:06d}_{ear_avg:.3f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert saved: {alert_count}, Fatigue saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"‚úÖ Image dataset preparation finished.\")\n",
        "    print(f\"Total images saved: Alert={alert_count}, Fatigue={fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "    # Check if we actually generated any fatigue images\n",
        "    if fatigue_count == 0 and alert_count == 0:\n",
        "        print(\"Error: No images were generated. Check video path and dlib predictor.\")\n",
        "        return False\n",
        "    elif fatigue_count == 0:\n",
        "        print(\"Warning: Only ALERT (0) images were generated. CNN training results will be poor.\")\n",
        "        print(\"Try using a video with more eye closure/blinks, or lower the EAR_THRESHOLD.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 2. MODEL TRAINING FUNCTIONS (Images -> Model) ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "def load_data(data_directory, img_size, batch_size):\n",
        "    \"\"\"Loads and prepares the image dataset from the directory structure.\"\"\"\n",
        "    print(\"\\n--- 2.1. Loading Data from Disk ---\")\n",
        "\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary', # Output is 0 or 1\n",
        "        image_size=img_size,\n",
        "        interpolation='bilinear',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    normalization_layer = Rescaling(1./255)\n",
        "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    if dataset_size == 0:\n",
        "        print(f\"Error: No batches found in '{data_directory}'. Cannot train LFB model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split dataset: 80% train, 20% validation\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    train_ds = dataset.take(train_size)\n",
        "    val_ds = dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Total batches found: {dataset_size}. Split: Train={train_size}, Validation={dataset_size - train_size}\")\n",
        "\n",
        "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines the LFB CNN model architecture.\"\"\"\n",
        "    print(\"--- 2.2. Defining LFB CNN Model Architecture ---\")\n",
        "    model = Sequential([\n",
        "        # First Block\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Classification layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_lfb_model(train_ds, val_ds):\n",
        "    \"\"\"Compiles and trains the defined LFB CNN model.\"\"\"\n",
        "\n",
        "    input_shape = IMAGE_SIZE + (3,)\n",
        "    model = create_cnn_model(input_shape)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    print(f\"--- 2.3. Starting LFB Model Training for {EPOCHS} epochs ---\")\n",
        "\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------\n",
        "# --- 3. MAIN EXECUTION ---\n",
        "# ----------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # FIX: Remove the Jupyter/Colab kernel arguments (-f <filename>)\n",
        "    # that cause the 'unrecognized arguments' error in argparse.\n",
        "    if '-f' in sys.argv:\n",
        "        f_index = sys.argv.index('-f')\n",
        "        # Remove the '-f' flag and the following argument (the json file path)\n",
        "        sys.argv = sys.argv[:f_index] + sys.argv[f_index+2:]\n",
        "\n",
        "    # ‚ö†Ô∏è Set up the arguments and path. This allows running it from command line,\n",
        "    # but defaults to the hardcoded path above.\n",
        "    parser = argparse.ArgumentParser(description=\"Full LFB Drowsiness Detection Training Pipeline.\")\n",
        "    parser.add_argument(\"--video\", type=str, default=DEFAULT_VIDEO_PATH,\n",
        "                        help=\"Path to the input video file.\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=DLIB_PREDICTOR_PATH,\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- Step 1: Data Generation ---\n",
        "    success = generate_cnn_dataset(args.video, args.predictor, DATA_DIR)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\nPipeline failed at the data generation step. Please check the video and predictor paths.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Step 2: Model Training ---\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        train_data, val_data = load_data(DATA_DIR, IMAGE_SIZE, BATCH_SIZE)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_lfb_model(train_data, val_data)\n",
        "\n",
        "        # --- Save the model (FULL MODEL) ---\n",
        "        trained_model.save(LFB_MODEL_SAVE_PATH)\n",
        "\n",
        "        # --- Save the architecture as YAML (via JSON FIX) ---\n",
        "        # The .to_json() method is generally available on the Sequential model object.\n",
        "        # We save this JSON output to a .yaml file to satisfy the user's request.\n",
        "        json_model_string = trained_model.to_json()\n",
        "        with open(LFB_MODEL_YAML_PATH, \"w\") as yaml_file:\n",
        "            # Note: This file will contain JSON content, but use the .yaml extension.\n",
        "            yaml_file.write(json_model_string)\n",
        "\n",
        "        print(\"\\n---------------------------------------------------------\")\n",
        "        print(f\"‚úÖ FINAL SUCCESS: LFB Model (Full) saved to: {os.path.abspath(LFB_MODEL_SAVE_PATH)}\")\n",
        "        print(f\"‚úÖ LFB Model (YAML Architecture - contains JSON) saved to: {os.path.abspath(LFB_MODEL_YAML_PATH)}\")\n",
        "        print(\"---------------------------------------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during model training (Step 2): {e}\")\n",
        "        print(\"Please verify the 'cnn_dataset' folder structure.\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvjHjCkQF2YZ"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x_6cTm-E9JF"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "tcdAdMxI31YI",
        "outputId": "25c62073-be0a-4cd9-d61b-f8601a06d1db"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'audio_classifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1963313115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmediapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/tasks/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/tasks/python/audio/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mAudioClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mAudioClassifierOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioClassifierOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mAudioClassifierResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioClassifierResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'audio_classifier' is not defined"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "\n",
        "# ----- Step 1: Extract ZIPs -----\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "eye_zip = r\"/content/drive/MyDrive/data_set/archive (1).zip\"\n",
        "\n",
        "extract_yawn = r\"C:\\Users\\User\\Downloads\\yawn_dataset\"\n",
        "extract_eye = r\"C:\\Users\\User\\Downloads\\eye_blink_dataset\"\n",
        "\n",
        "if not os.path.exists(extract_yawn):\n",
        "    with zipfile.ZipFile(yawn_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_yawn)\n",
        "\n",
        "if not os.path.exists(extract_eye):\n",
        "    with zipfile.ZipFile(eye_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_eye)\n",
        "\n",
        "print(\"‚úÖ Datasets extracted successfully!\\n\")\n",
        "\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    h, w = image.shape[:2]\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "\n",
        "    if part == \"eye\":\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE)\n",
        "    else:\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH)\n",
        "    return [feature]\n",
        "\n",
        "def load_dataset(path, part=\"eye\"):\n",
        "    X, y = [], []\n",
        "    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "    for label, folder in enumerate(subfolders):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            feature = extract_features(img, part)\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ----- Step 3: Train Eye Blink Model -----\n",
        "print(\"üîπ Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_dataset(extract_eye, part=\"eye\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_eye, y_eye, test_size=0.2)\n",
        "eye_model = SVC(kernel='linear')\n",
        "eye_model.fit(X_train, y_train)\n",
        "print(\"Eye Blink Accuracy:\", accuracy_score(y_test, eye_model.predict(X_test)))\n",
        "joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "print(\"‚úÖ Saved: eye_blink_model.pkl\")\n",
        "\n",
        "# ----- Step 4: Train Yawn Model -----\n",
        "print(\"\\nüîπ Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2)\n",
        "yawn_model = SVC(kernel='linear')\n",
        "yawn_model.fit(X_train, y_train)\n",
        "print(\"Yawn Detection Accuracy:\", accuracy_score(y_test, yawn_model.predict(X_test)))\n",
        "joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "print(\"‚úÖ Saved: yawn_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X13uEoxr4bZq",
        "outputId": "279e0924-79d2-4d23-812a-642272acf18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from Mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->Mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->Mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->Mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->Mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->Mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->Mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->Mediapipe) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install Mediapipe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
