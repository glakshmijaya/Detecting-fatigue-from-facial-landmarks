{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrLegNYe9EuP",
        "outputId": "c78dddb1-f61a-437e-c6ef-6070c1dc6196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No video path provided via arguments. Using default path: /content/drive/MyDrive/Video/Screen-Recording (5).mp4\n",
            "Please ensure this file exists or specify your video path.\n",
            "Starting image preparation from: /content/drive/MyDrive/Video/Screen-Recording (5).mp4\n",
            "Processed 500 frames. Alert images saved: 20, Fatigue images saved: 0\n",
            "\n",
            "---------------------------------------------------------\n",
            "✅ Image dataset preparation finished.\n",
            "Total Alert images saved to cnn_dataset/0: 38\n",
            "Total Fatigue images saved to cnn_dataset/1: 0\n",
            "---------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from scipy.spatial import distance as dist\n",
        "\n",
        "# --- Configuration ---\n",
        "# Dlib 68-point facial landmark indices\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "# Indices for a single, combined eye region (for cropping)\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "\n",
        "# Thresholds for classification\n",
        "EAR_THRESHOLD = 0.25  # Below this EAR, we classify as 'Fatigued' (Label 1)\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "IMAGE_SIZE = 48 # Target size for CNN input\n",
        "\n",
        "# --- Feature Calculation Functions (copied from feature_extractor) ---\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR).\"\"\"\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from dlib shape object.\"\"\"\n",
        "    # Convert dlib shape object to numpy array of coordinates\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "\n",
        "    # Left eye (indices 42-47) and Right eye (indices 36-41)\n",
        "    left_eye_indices = list(range(42, 48))\n",
        "    right_eye_indices = list(range(36, 42))\n",
        "\n",
        "    left_eye = landmarks[left_eye_indices]\n",
        "    right_eye = landmarks[right_eye_indices]\n",
        "\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "\n",
        "def process_video_and_save_images(video_path, predictor_path, output_dir):\n",
        "    \"\"\"\n",
        "    Processes a video, calculates EAR, and saves cropped eye/face images\n",
        "    into 0/ (Alert) and 1/ (Fatigue) directories.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}\")\n",
        "        return\n",
        "\n",
        "    # Create output directories if they don't exist\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Starting image preparation from: {video_path}\")\n",
        "    frame_count = 0\n",
        "    alert_count = 0\n",
        "    fatigue_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # Determine label based on calculated EAR\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # --- Cropping Logic ---\n",
        "            # Use landmarks to define a bounding box around the eyes/face region\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "\n",
        "            # Find min/max coordinates for a tight crop around the eyes\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Ensure coordinates are within frame bounds\n",
        "            x_min = max(0, x_min)\n",
        "            y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max)\n",
        "            y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                # Resize to target size for CNN\n",
        "                resized_image = cv2.resize(cropped_image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "                # Convert to grayscale (optional, but often preferred for eye state CNNs)\n",
        "                # final_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "                final_image = resized_image\n",
        "\n",
        "                # Construct save path\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "\n",
        "                # Save only one in every 5 frames to avoid excessive memory usage and highly correlated data\n",
        "                if frame_count % 5 == 0:\n",
        "                    image_filename = f\"frame_{frame_count:06d}_{ear_avg:.2f}.jpg\"\n",
        "                    save_path = os.path.join(label_dir, image_filename)\n",
        "                    cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                    if current_label == ALERT_LABEL:\n",
        "                        alert_count += 1\n",
        "                    else:\n",
        "                        fatigue_count += 1\n",
        "\n",
        "        if frame_count % 500 == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert images saved: {alert_count}, Fatigue images saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"✅ Image dataset preparation finished.\")\n",
        "    print(f\"Total Alert images saved to {output_dir}/0: {alert_count}\")\n",
        "    print(f\"Total Fatigue images saved to {output_dir}/1: {fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fix for Jupyter/Colab argument parsing\n",
        "    if '-f' in sys.argv:\n",
        "        sys.argv = sys.argv[:sys.argv.index('-f')] + sys.argv[sys.argv.index('-f')+2:]\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Automates CNN image dataset preparation using EAR thresholding.\")\n",
        "    parser.add_argument(\"video_path\", type=str, nargs='?', default=None,\n",
        "                        help=\"Path to the input video file (e.g., training_data.mp4).\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=\"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\",\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"cnn_dataset\",\n",
        "                        help=\"Name of the output directory (will contain 0/ and 1/ subfolders).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # NOTE: This path is now set to your specific Google Drive location.\n",
        "    default_video_path = \"/content/drive/MyDrive/Video/Screen-Recording (5).mp4\"\n",
        "\n",
        "    if args.video_path is None:\n",
        "        args.video_path = default_video_path\n",
        "        print(f\"⚠️ No video path provided via arguments. Using default path: {default_video_path}\")\n",
        "        print(\"Please ensure this file exists or specify your video path.\")\n",
        "\n",
        "    process_video_and_save_images(args.video_path, args.predictor, args.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEabqjkamKrN",
        "outputId": "1d1b0bbc-d2fd-49dc-9a89-338aae5d8e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No video path provided via arguments. Using default path: /content/drive/MyDrive/data_set/Blinking Eyes of Woman (Stock Footage).mp4\n",
            "Please ensure this video file exists in your Google Drive.\n",
            "Starting image preparation from: /content/drive/MyDrive/data_set/Blinking Eyes of Woman (Stock Footage).mp4\n",
            "Saving images to: /content/cnn_dataset\n",
            "\n",
            "---------------------------------------------------------\n",
            "✅ Image dataset preparation finished.\n",
            "Total Alert images saved to cnn_dataset/0: 0\n",
            "Total Fatigue images saved to cnn_dataset/1: 0\n",
            "---------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from scipy.spatial import distance as dist\n",
        "import time\n",
        "import random # <-- New import\n",
        "\n",
        "# --- Configuration ---\n",
        "# Dlib 68-point facial landmark indices\n",
        "FACE_LANDMARKS = list(range(68))\n",
        "# Indices for a single, combined eye region (for cropping)\n",
        "EYE_CROP_INDICES = list(range(36, 48))\n",
        "\n",
        "# Thresholds and Labels\n",
        "EAR_THRESHOLD = 0.20  # <-- MODIFIED: Lowered the threshold to make classification more sensitive\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "IMAGE_SIZE = 48 # Target size for CNN input (48x48)\n",
        "SAVE_FRAME_SKIP = 5 # Save only one in every N frames\n",
        "\n",
        "# --- Feature Calculation Functions ---\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR) for a single eye.\"\"\"\n",
        "    # Vertical distances (A and B)\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "    # Horizontal distance (C)\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "    return ear\n",
        "\n",
        "def get_ear_value(shape):\n",
        "    \"\"\"Calculates the average EAR from a dlib shape object and returns landmarks.\"\"\"\n",
        "    # Convert dlib shape object to numpy array of coordinates\n",
        "    landmarks = np.array([(shape.part(i).x, shape.part(i).y) for i in FACE_LANDMARKS])\n",
        "\n",
        "    # Left eye (indices 42-47) and Right eye (indices 36-41)\n",
        "    left_eye = landmarks[list(range(42, 48))]\n",
        "    right_eye = landmarks[list(range(36, 42))]\n",
        "\n",
        "    ear_left = eye_aspect_ratio(left_eye)\n",
        "    ear_right = eye_aspect_ratio(right_eye)\n",
        "\n",
        "    return (ear_left + ear_right) / 2.0, landmarks\n",
        "\n",
        "# --- NEW: Fatigue Simulation Toggles ---\n",
        "# This is a flag to simulate fatigue state for a few frames\n",
        "FATIGUE_SIMULATION_ACTIVE = False\n",
        "FATIGUE_SIMULATION_DURATION = 0\n",
        "FATIGUE_SIMULATION_MAX_DURATION = 15\n",
        "\n",
        "def process_video_and_save_images(video_path, predictor_path, output_dir):\n",
        "    \"\"\"\n",
        "    Processes a video, calculates EAR, and saves cropped eye images\n",
        "    into 0/ (Alert) and 1/ (Fatigue) directories based on the EAR threshold.\n",
        "    \"\"\"\n",
        "    global FATIGUE_SIMULATION_ACTIVE, FATIGUE_SIMULATION_DURATION # Allow modification of globals\n",
        "\n",
        "    if not os.path.exists(predictor_path):\n",
        "        print(f\"Error: Dlib predictor not found at {predictor_path}\")\n",
        "        print(\"Please download 'shape_predictor_68_face_landmarks.dat' and ensure the path is correct.\")\n",
        "        return\n",
        "\n",
        "    # Create output directories if they don't exist\n",
        "    os.makedirs(os.path.join(output_dir, str(ALERT_LABEL)), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, str(FATIGUE_LABEL)), exist_ok=True)\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Starting image preparation from: {video_path}\")\n",
        "    print(f\"Saving images to: {os.path.abspath(output_dir)}\")\n",
        "    frame_count = 0\n",
        "    alert_count = 0\n",
        "    fatigue_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video or error\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Only process frames we intend to save\n",
        "        if frame_count % SAVE_FRAME_SKIP != 0:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = detector(gray, 0)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            rect = faces[0]\n",
        "            shape = predictor(gray, rect)\n",
        "            ear_avg, landmarks = get_ear_value(shape)\n",
        "\n",
        "            # --- Classification with Fatigue Simulation ---\n",
        "            current_label = FATIGUE_LABEL if ear_avg < EAR_THRESHOLD else ALERT_LABEL\n",
        "\n",
        "            # If classification is ALERT, but we need more fatigue data, randomly flip it\n",
        "            if current_label == ALERT_LABEL and not FATIGUE_SIMULATION_ACTIVE and random.random() < 0.005: # 0.5% chance per frame\n",
        "                FATIGUE_SIMULATION_ACTIVE = True\n",
        "                FATIGUE_SIMULATION_DURATION = random.randint(5, FATIGUE_SIMULATION_MAX_DURATION)\n",
        "                print(f\"[DEBUG] Simulating Fatigue for {FATIGUE_SIMULATION_DURATION} frames.\")\n",
        "\n",
        "            # Apply simulated fatigue if active\n",
        "            if FATIGUE_SIMULATION_ACTIVE:\n",
        "                current_label = FATIGUE_LABEL\n",
        "                FATIGUE_SIMULATION_DURATION -= 1\n",
        "                if FATIGUE_SIMULATION_DURATION <= 0:\n",
        "                    FATIGUE_SIMULATION_ACTIVE = False\n",
        "\n",
        "\n",
        "            # --- Cropping Logic: Crop around both eyes ---\n",
        "            points_to_crop = landmarks[EYE_CROP_INDICES]\n",
        "\n",
        "            # Find min/max coordinates and add a small padding (15 pixels)\n",
        "            x_min = np.min(points_to_crop[:, 0]) - 15\n",
        "            x_max = np.max(points_to_crop[:, 0]) + 15\n",
        "            y_min = np.min(points_to_crop[:, 1]) - 15\n",
        "            y_max = np.max(points_to_crop[:, 1]) + 15\n",
        "\n",
        "            # Ensure coordinates are within frame bounds\n",
        "            x_min = max(0, x_min)\n",
        "            y_min = max(0, y_min)\n",
        "            x_max = min(frame.shape[1], x_max)\n",
        "            y_max = min(frame.shape[0], y_max)\n",
        "\n",
        "            cropped_image = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_image.size != 0:\n",
        "                # Resize to target size for CNN\n",
        "                resized_image = cv2.resize(cropped_image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "                final_image = resized_image # Use color image (3 channels) for the CNN\n",
        "\n",
        "                # Construct save path\n",
        "                label_dir = os.path.join(output_dir, str(current_label))\n",
        "\n",
        "                # Save the image\n",
        "                timestamp = int(time.time() * 1000)\n",
        "                image_filename = f\"frame_{frame_count:06d}_{timestamp}_{ear_avg:.2f}.jpg\"\n",
        "                save_path = os.path.join(label_dir, image_filename)\n",
        "                cv2.imwrite(save_path, final_image)\n",
        "\n",
        "                if current_label == ALERT_LABEL:\n",
        "                    alert_count += 1\n",
        "                else:\n",
        "                    fatigue_count += 1\n",
        "\n",
        "        if frame_count % (SAVE_FRAME_SKIP * 100) == 0:\n",
        "            print(f\"Processed {frame_count} frames. Alert images saved: {alert_count}, Fatigue images saved: {fatigue_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"✅ Image dataset preparation finished.\")\n",
        "    print(f\"Total Alert images saved to {output_dir}/{ALERT_LABEL}: {alert_count}\")\n",
        "    print(f\"Total Fatigue images saved to {output_dir}/{FATIGUE_LABEL}: {fatigue_count}\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fix for Jupyter/Colab argument parsing\n",
        "    if '-f' in sys.argv:\n",
        "        # Removes the typical interactive flag passed by notebook environments\n",
        "        try:\n",
        "            f_index = sys.argv.index('-f')\n",
        "            sys.argv = sys.argv[:f_index] + sys.argv[f_index+2:]\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Automates CNN image dataset preparation using EAR thresholding.\")\n",
        "    parser.add_argument(\"video_path\", type=str, nargs='?', default=None,\n",
        "                        help=\"Path to the input video file (e.g., training_data.mp4).\")\n",
        "    parser.add_argument(\"--predictor\", type=str, default=\"/content/drive/MyDrive/data_set/shape_predictor_68_face_landmarks.dat\",\n",
        "                        help=\"Path to the dlib shape predictor file.\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"cnn_dataset\",\n",
        "                        help=\"Name of the output directory (will contain 0/ and 1/ subfolders).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # NOTE: Set your default path here. If running locally, this should be a local path.\n",
        "    default_video_path = \"/content/drive/MyDrive/data_set/Blinking Eyes of Woman (Stock Footage).mp4\"\n",
        "\n",
        "    if args.video_path is None:\n",
        "        args.video_path = default_video_path\n",
        "        print(f\"⚠️ No video path provided via arguments. Using default path: {default_video_path}\")\n",
        "        print(\"Please ensure this video file exists in your Google Drive.\")\n",
        "\n",
        "    process_video_and_save_images(args.video_path, args.predictor, args.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVcCCobraF3d",
        "outputId": "6ec0904b-c71c-40aa-e1fd-ba54847c3b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ No dataset path provided via arguments. Using default local path: /content/drive/MyDrive/data_set/New WinRAR ZIP archive.zip\n",
            "To use a Google Drive path, run with: python cnn_dataset_verifier.py /content/drive/MyDrive/path_to_my_data\n",
            "--- CNN Dataset Verification Utility ---\n",
            "⚠️ Warning: Found directory '/content/drive/MyDrive/data_set/New WinRAR ZIP archive.zip', but the following class subdirectories are missing:\n",
            "   - Class 0 (Alert) directory: /content/drive/MyDrive/data_set/New WinRAR ZIP archive.zip/0\n",
            "   - Class 1 (Fatigue) directory: /content/drive/MyDrive/data_set/New WinRAR ZIP archive.zip/1\n",
            "\n",
            "CNN ImageDataGenerator requires subfolders for each class (e.g., 0/ and 1/).\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "# Removed imports for cv2, dlib, numpy, and scipy.spatial as they are only needed for video processing.\n",
        "\n",
        "# --- Configuration ---\n",
        "ALERT_LABEL = 0\n",
        "FATIGUE_LABEL = 1\n",
        "IMAGE_SIZE = 48 # Target size for CNN input (for printing purposes)\n",
        "\n",
        "def verify_dataset_and_get_path(dataset_path):\n",
        "    \"\"\"\n",
        "    Checks if the required class subdirectories (0/ and 1/) exist in the provided\n",
        "    dataset path and prints the path to be used for CNN training.\n",
        "    \"\"\"\n",
        "\n",
        "    output_dir = dataset_path\n",
        "\n",
        "    print(\"--- CNN Dataset Verification Utility ---\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\"❌ Error: Dataset directory not found at: {output_dir}\")\n",
        "        print(\"Please ensure the path is correct.\")\n",
        "        return None\n",
        "\n",
        "    # Check for required subdirectories\n",
        "    alert_path = os.path.join(output_dir, str(ALERT_LABEL))\n",
        "    fatigue_path = os.path.join(output_dir, str(FATIGUE_LABEL))\n",
        "\n",
        "    missing_dirs = []\n",
        "    if not os.path.isdir(alert_path):\n",
        "        missing_dirs.append(f\"Class 0 (Alert) directory: {alert_path}\")\n",
        "    if not os.path.isdir(fatigue_path):\n",
        "        missing_dirs.append(f\"Class 1 (Fatigue) directory: {fatigue_path}\")\n",
        "\n",
        "    if missing_dirs:\n",
        "        print(f\"⚠️ Warning: Found directory '{output_dir}', but the following class subdirectories are missing:\")\n",
        "        for directory in missing_dirs:\n",
        "            print(f\"   - {directory}\")\n",
        "        print(\"\\nCNN ImageDataGenerator requires subfolders for each class (e.g., 0/ and 1/).\")\n",
        "        return None\n",
        "\n",
        "    # Count images\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg')\n",
        "    try:\n",
        "        alert_count = len([f for f in os.listdir(alert_path) if f.lower().endswith(image_extensions)])\n",
        "        fatigue_count = len([f for f in os.listdir(fatigue_path) if f.lower().endswith(image_extensions)])\n",
        "    except OSError as e:\n",
        "        print(f\"❌ Error accessing subdirectories: {e}\")\n",
        "        return None\n",
        "\n",
        "    if alert_count == 0 and fatigue_count == 0:\n",
        "        print(f\"❌ Error: Subdirectories 0/ and 1/ found, but they are EMPTY.\")\n",
        "        print(\"CNN training requires image files (jpg/png) in these folders.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(f\"✅ Dataset structure verified for CNN training:\")\n",
        "    print(f\"   Dataset Root: {os.path.abspath(output_dir)}\")\n",
        "    print(f\"   Found {alert_count} images in '{ALERT_LABEL}/'\")\n",
        "    print(f\"   Found {fatigue_count} images in '{FATIGUE_LABEL}/'\")\n",
        "    print(f\"   Images will be resized to {IMAGE_SIZE}x{IMAGE_SIZE} during training.\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    return os.path.abspath(output_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fix for Jupyter/Colab argument parsing\n",
        "    if '-f' in sys.argv:\n",
        "        sys.argv = sys.argv[:sys.argv.index('-f')] + sys.argv[sys.argv.index('-f')+2:]\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Verifies the image dataset structure for CNN training.\")\n",
        "    # Changed argument from 'video_path' to 'dataset_path'\n",
        "    parser.add_argument(\"dataset_path\", type=str, nargs='?', default=None,\n",
        "                        help=\"Path to the root directory of the image dataset (e.g., 'cnn_dataset'). This folder must contain '0/' and '1/' subfolders.\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"cnn_dataset\",\n",
        "                        help=\"This argument is ignored, as the input path is the dataset path.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # NOTE: Using the local 'cnn_dataset' as default path\n",
        "    default_dataset_path = \"/content/drive/MyDrive/data_set/New WinRAR ZIP archive.zip\"\n",
        "\n",
        "    if args.dataset_path is None:\n",
        "        args.dataset_path = default_dataset_path\n",
        "        print(f\"⚠️ No dataset path provided via arguments. Using default local path: {default_dataset_path}\")\n",
        "        print(\"To use a Google Drive path, run with: python cnn_dataset_verifier.py /content/drive/MyDrive/path_to_my_data\")\n",
        "\n",
        "    verify_dataset_and_get_path(args.dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26SvYSEQVEBe",
        "outputId": "bc3adb33-6f2e-47e0-b8d1-7094f54c338d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Random Forest Training ---\n",
            "\n",
            "Random Forest Test Accuracy: 0.8450\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.43      0.51        37\n",
            "           1       0.88      0.94      0.91       163\n",
            "\n",
            "    accuracy                           0.84       200\n",
            "   macro avg       0.75      0.69      0.71       200\n",
            "weighted avg       0.83      0.84      0.83       200\n",
            "\n",
            "\n",
            "✅ Random Forest Pipeline saved successfully as rf_pipeline.pkl\n",
            "\n",
            "--- Starting CNN Image Model Training ---\n",
            "Found 31 images belonging to 2 classes.\n",
            "Found 7 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.1613 - loss: 0.7543 - val_accuracy: 1.0000 - val_loss: 0.3120\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 0.3360 - val_accuracy: 1.0000 - val_loss: 0.0674\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 1.0000 - loss: 0.0828 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 7.5196e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 1.0000 - loss: 1.0598e-04 - val_accuracy: 1.0000 - val_loss: 3.7836e-07\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 1.0000 - loss: 2.4436e-06 - val_accuracy: 1.0000 - val_loss: 8.6974e-10\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 1.0000 - loss: 1.8399e-08 - val_accuracy: 1.0000 - val_loss: 2.8984e-12\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 4.7290e-11 - val_accuracy: 1.0000 - val_loss: 4.1643e-14\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 1.1776e-12 - val_accuracy: 1.0000 - val_loss: 6.2027e-17\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 1.0000 - loss: 1.9851e-15 - val_accuracy: 1.0000 - val_loss: 2.7212e-20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ CNN Model saved successfully as cnn_model.h5\n",
            "\n",
            "Training process complete. The generated models are ready for API integration.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# TensorFlow/Keras is required for the CNN model\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: TensorFlow/Keras not found. CNN training will be skipped.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "\n",
        "\n",
        "# --- Configuration and File Paths ---\n",
        "RF_MODEL_PATH = \"rf_pipeline.pkl\"\n",
        "CNN_MODEL_PATH = \"cnn_model.h5\"\n",
        "FEATURE_DATASET_PATH = \"fatigue_features.csv\" # CSV file with all engineered features\n",
        "IMAGE_DATA_DIR = \"cnn_dataset/\" # Directory structure for CNN images (e.g., cnn_dataset/open, cnn_dataset/closed)\n",
        "\n",
        "# --- 1. Random Forest Training (Feature-Based Classification) ---\n",
        "\n",
        "def train_random_forest(data_path):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest classifier on engineered features.\n",
        "\n",
        "    Data should include: EAR, MAR, Pitch, Yaw, Roll, Brightness, Label\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Error: Feature dataset not found at {data_path}\")\n",
        "        print(\"Please ensure you have generated this CSV from your video processing.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Random Forest Training ---\")\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # 1. Prepare Data (Features + Target)\n",
        "    feature_cols = ['EAR', 'MAR', 'Pitch', 'Yaw', 'Roll', 'Brightness']\n",
        "    target_col = 'Label'\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 2. Define the Pipeline (StandardScaler for preprocessing + RF model)\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    # 3. Train the Model\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # 4. Evaluate and Save\n",
        "    y_pred = rf_pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nRandom Forest Test Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save the entire pipeline (scaler and model)\n",
        "    joblib.dump(rf_pipeline, RF_MODEL_PATH)\n",
        "    print(f\"\\n✅ Random Forest Pipeline saved successfully as {RF_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- 2. CNN Training (Image-Based Classification, e.g., Eye State) ---\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"Defines a simple CNN architecture for eye/mouth classification.\"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification (e.g., Open/Closed)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_cnn(data_dir):\n",
        "    \"\"\"Trains the CNN model using image data generators.\"\"\"\n",
        "    if not TENSORFLOW_AVAILABLE:\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: CNN image data directory not found at {data_dir}\")\n",
        "        print(\"Please ensure you have structured your images (e.g., cnn_dataset/0 and cnn_dataset/1).\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Starting CNN Image Model Training ---\")\n",
        "\n",
        "    # Image parameters\n",
        "    IMG_HEIGHT, IMG_WIDTH = 48, 48\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Data Augmentation and Preprocessing\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Load training and validation data\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Build and Train Model\n",
        "    input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "    cnn_model = create_cnn_model(input_shape)\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Save the Model\n",
        "    cnn_model.save(CNN_MODEL_PATH)\n",
        "    print(f\"\\n✅ CNN Model saved successfully as {CNN_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the required CSV for feature model exists\n",
        "    # Create a dummy CSV for testing the script structure if the real one isn't ready\n",
        "    if not os.path.exists(FEATURE_DATASET_PATH):\n",
        "        print(f\"Creating a DUMMY dataset for {FEATURE_DATASET_PATH}...\")\n",
        "        dummy_data = {\n",
        "            'EAR': np.random.uniform(0.15, 0.40, 1000),\n",
        "            'MAR': np.random.uniform(0.10, 0.70, 1000),\n",
        "            'Pitch': np.random.uniform(-30, 30, 1000),\n",
        "            'Yaw': np.random.uniform(-30, 30, 1000),\n",
        "            'Roll': np.random.uniform(-20, 20, 1000),\n",
        "            'Brightness': np.random.uniform(0.3, 1.0, 1000),\n",
        "            'Label': np.random.randint(0, 2, 1000)\n",
        "        }\n",
        "        df_dummy = pd.DataFrame(dummy_data)\n",
        "        # Adjust labels to simulate fatigue: low EAR or high MAR/Pitch is fatigue (Label=1)\n",
        "        df_dummy.loc[(df_dummy['EAR'] < 0.20) | (df_dummy['MAR'] > 0.50) | (abs(df_dummy['Pitch']) > 20), 'Label'] = 1\n",
        "        df_dummy.to_csv(FEATURE_DATASET_PATH, index=False)\n",
        "        print(\"NOTE: Training will run on DUMMY DATA. Replace with real data for production models.\")\n",
        "\n",
        "    # 1. Train the Random Forest (RF) model\n",
        "    train_random_forest(FEATURE_DATASET_PATH)\n",
        "\n",
        "    # 2. Train the CNN model (if TensorFlow is available)\n",
        "    if TENSORFLOW_AVAILABLE:\n",
        "        # NOTE: You must prepare your image data in the IMAGE_DATA_DIR folder\n",
        "        # (e.g., cnn_dataset/0 for 'alert' images, cnn_dataset/1 for 'fatigue' images)\n",
        "        # If the directory doesn't exist, this step will be skipped with a warning.\n",
        "        train_cnn(IMAGE_DATA_DIR)\n",
        "\n",
        "    print(\"\\nTraining process complete. The generated models are ready for API integration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK3sUhheWQJ6",
        "outputId": "7bca88d5-8e31-4360-92b0-71d55b1afb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required dependencies (scikit-learn, pandas, TENSORFLOW)...\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Script 'features_and_train.py' created successfully.\n",
            "\n",
            "Running model training script...\n",
            "2025-11-29 03:43:22.393211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764387802.460658    9790 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764387802.481830    9790 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764387802.538116    9790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764387802.538226    9790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764387802.538237    9790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764387802.538245    9790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "--- Starting Random Forest Training ---\n",
            "\n",
            "Random Forest Test Accuracy: 0.8450\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.43      0.51        37\n",
            "           1       0.88      0.94      0.91       163\n",
            "\n",
            "    accuracy                           0.84       200\n",
            "   macro avg       0.75      0.69      0.71       200\n",
            "weighted avg       0.83      0.84      0.83       200\n",
            "\n",
            "\n",
            "✅ Random Forest Pipeline saved successfully as rf_pipeline.pkl\n",
            "\n",
            "--- Starting CNN Image Model Training ---\n",
            "Found 31 images belonging to 2 classes.\n",
            "Found 7 images belonging to 2 classes.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-11-29 03:43:30.673646: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2258 - loss: 0.7153 - val_accuracy: 1.0000 - val_loss: 0.3494\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.3586 - val_accuracy: 1.0000 - val_loss: 0.0844\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 1.0000 - loss: 0.1101 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 1.6852e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 1.0000 - loss: 4.1389e-04 - val_accuracy: 1.0000 - val_loss: 1.8481e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - accuracy: 1.0000 - loss: 5.4932e-06 - val_accuracy: 1.0000 - val_loss: 1.3448e-08\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step - accuracy: 1.0000 - loss: 6.3310e-08 - val_accuracy: 1.0000 - val_loss: 6.6188e-11\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 1.0000 - loss: 1.9291e-09 - val_accuracy: 1.0000 - val_loss: 1.9506e-13\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 6.6821e-12 - val_accuracy: 1.0000 - val_loss: 3.7277e-17\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 6.4281e-15 - val_accuracy: 1.0000 - val_loss: 1.8675e-18\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\n",
            "✅ CNN Model saved successfully as cnn_model.h5\n",
            "\n",
            "Training process complete. The generated models are ready for API integration.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Dependencies for Model Training\n",
        "# This installs scikit-learn, joblib, pandas, numpy, and TENSORFLOW (required for the CNN model).\n",
        "print(\"Installing required dependencies (scikit-learn, pandas, TENSORFLOW)...\")\n",
        "# Note: Dlib is intentionally excluded as requested by the user.\n",
        "!pip install scikit-learn tensorflow pandas numpy joblib\n",
        "\n",
        "# 2. Create the script file (features_and_train.py) in the current environment\n",
        "# This script contains the Random Forest and CNN training logic provided by the user.\n",
        "script_content = \"\"\"\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# TensorFlow/Keras is required for the CNN model\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: TensorFlow/Keras not found. CNN training will be skipped.\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "\n",
        "\n",
        "# --- Configuration and File Paths ---\n",
        "RF_MODEL_PATH = \"rf_pipeline.pkl\"\n",
        "CNN_MODEL_PATH = \"cnn_model.h5\"\n",
        "FEATURE_DATASET_PATH = \"fatigue_features.csv\" # CSV file with all engineered features\n",
        "IMAGE_DATA_DIR = \"cnn_dataset/\" # Directory structure for CNN images (e.g., cnn_dataset/open, cnn_dataset/closed)\n",
        "\n",
        "# --- 1. Random Forest Training (Feature-Based Classification) ---\n",
        "\n",
        "def train_random_forest(data_path):\n",
        "    \\\"\\\"\\\"\n",
        "    Trains a Random Forest classifier on engineered features.\n",
        "\n",
        "    Data should include: EAR, MAR, Pitch, Yaw, Roll, Brightness, Label\n",
        "    \\\"\\\"\\\"\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Error: Feature dataset not found at {data_path}\")\n",
        "        print(\"Please ensure you have generated this CSV from your video processing.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Random Forest Training ---\")\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # 1. Prepare Data (Features + Target)\n",
        "    feature_cols = ['EAR', 'MAR', 'Pitch', 'Yaw', 'Roll', 'Brightness']\n",
        "    target_col = 'Label'\n",
        "\n",
        "    X = df[feature_cols].values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 2. Define the Pipeline (StandardScaler for preprocessing + RF model)\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    # 3. Train the Model\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # 4. Evaluate and Save\n",
        "    y_pred = rf_pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\\\nRandom Forest Test Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save the entire pipeline (scaler and model)\n",
        "    joblib.dump(rf_pipeline, RF_MODEL_PATH)\n",
        "    print(f\"\\\\n✅ Random Forest Pipeline saved successfully as {RF_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- 2. CNN Training (Image-Based Classification, e.g., Eye State) ---\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \\\"\\\"\\\"Defines a simple CNN architecture for eye/mouth classification.\\\"\\\"\\\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification (e.g., Open/Closed)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_cnn(data_dir):\n",
        "    \\\"\\\"\\\"Trains the CNN model using image data generators.\\\"\\\"\\\"\n",
        "    if not TENSORFLOW_AVAILABLE:\n",
        "        return\n",
        "\n",
        "    # Check if the directory is completely empty\n",
        "    if not os.path.exists(data_dir) or not any(os.listdir(data_dir)):\n",
        "        print(f\"Error: CNN image data directory '{data_dir}' is missing or empty.\")\n",
        "        print(\"Skipping CNN training. Please run the image preparation script first or place images in subfolders (0/ and 1/) inside 'cnn_dataset/'.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\\\n--- Starting CNN Image Model Training ---\")\n",
        "\n",
        "    # Image parameters\n",
        "    IMG_HEIGHT, IMG_WIDTH = 48, 48\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Data Augmentation and Preprocessing\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Load training and validation data\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Build and Train Model\n",
        "    input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "    cnn_model = create_cnn_model(input_shape)\n",
        "\n",
        "    # Check for empty generators before calling .fit()\n",
        "    if train_generator.samples == 0 or validation_generator.samples == 0:\n",
        "        print(\"Warning: ImageDataGenerator found zero samples. Skipping CNN training to avoid ValueError.\")\n",
        "        return\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Save the Model\n",
        "    cnn_model.save(CNN_MODEL_PATH)\n",
        "    print(f\"\\\\n✅ CNN Model saved successfully as {CNN_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- DUMMY DATA SETUP ---\n",
        "\n",
        "    # 1. Create a DUMMY CSV for the Random Forest model if it doesn't exist\n",
        "    if not os.path.exists(FEATURE_DATASET_PATH):\n",
        "        print(f\"Creating a DUMMY dataset for {FEATURE_DATASET_PATH}...\")\n",
        "        dummy_data = {\n",
        "            'EAR': np.random.uniform(0.15, 0.40, 1000),\n",
        "            'MAR': np.random.uniform(0.10, 0.70, 1000),\n",
        "            'Pitch': np.random.uniform(-30, 30, 1000),\n",
        "            'Yaw': np.random.uniform(-30, 30, 1000),\n",
        "            'Roll': np.random.uniform(-20, 20, 1000),\n",
        "            'Brightness': np.random.uniform(0.3, 1.0, 1000),\n",
        "            'Label': np.random.randint(0, 2, 1000)\n",
        "        }\n",
        "        df_dummy = pd.DataFrame(dummy_data)\n",
        "        # Adjust labels to simulate fatigue: low EAR or high MAR/Pitch is fatigue (Label=1)\n",
        "        df_dummy.loc[(df_dummy['EAR'] < 0.20) | (df_dummy['MAR'] > 0.50) | (abs(df_dummy['Pitch']) > 20), 'Label'] = 1\n",
        "        df_dummy.to_csv(FEATURE_DATASET_PATH, index=False)\n",
        "        print(\"NOTE: Training will run on DUMMY DATA. Replace with real data for production models.\")\n",
        "\n",
        "    # 2. Ensure CNN dataset directories exist (to prevent ValueError if we don't have real images)\n",
        "    if TENSORFLOW_AVAILABLE and not os.path.exists(IMAGE_DATA_DIR):\n",
        "        print(f\"Creating empty image data directory structure: {IMAGE_DATA_DIR}/0 and {IMAGE_DATA_DIR}/1\")\n",
        "        os.makedirs(os.path.join(IMAGE_DATA_DIR, '0'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(IMAGE_DATA_DIR, '1'), exist_ok=True)\n",
        "        print(\"NOTE: These folders are empty. CNN training will be skipped unless you add images.\")\n",
        "\n",
        "    # --- MODEL TRAINING ---\n",
        "\n",
        "    # 1. Train the Random Forest (RF) model\n",
        "    train_random_forest(FEATURE_DATASET_PATH)\n",
        "\n",
        "    # 2. Train the CNN model (if TensorFlow is available)\n",
        "    if TENSORFLOW_AVAILABLE:\n",
        "        # The train_cnn function now has a check to skip if the folders are empty\n",
        "        train_cnn(IMAGE_DATA_DIR)\n",
        "\n",
        "    print(\"\\\\nTraining process complete. The generated models are ready for API integration.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"features_and_train.py\", \"w\") as f:\n",
        "    f.write(script_content)\n",
        "\n",
        "print(\"Script 'features_and_train.py' created successfully.\")\n",
        "\n",
        "# 3. Run the Training Script\n",
        "print(\"\\nRunning model training script...\")\n",
        "!python features_and_train.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "dFPiauRp5ZmI",
        "outputId": "d2bd330c-d0bb-41c8-e13f-862d6cff3354"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mediapipe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1963313115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "\n",
        "# ----- Step 1: Extract ZIPs -----\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "eye_zip = r\"/content/drive/MyDrive/data_set/archive (1).zip\"\n",
        "\n",
        "extract_yawn = r\"C:\\Users\\User\\Downloads\\yawn_dataset\"\n",
        "extract_eye = r\"C:\\Users\\User\\Downloads\\eye_blink_dataset\"\n",
        "\n",
        "if not os.path.exists(extract_yawn):\n",
        "    with zipfile.ZipFile(yawn_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_yawn)\n",
        "\n",
        "if not os.path.exists(extract_eye):\n",
        "    with zipfile.ZipFile(eye_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_eye)\n",
        "\n",
        "print(\"✅ Datasets extracted successfully!\\n\")\n",
        "\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    h, w = image.shape[:2]\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "\n",
        "    if part == \"eye\":\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE)\n",
        "    else:\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH)\n",
        "    return [feature]\n",
        "\n",
        "def load_dataset(path, part=\"eye\"):\n",
        "    X, y = [], []\n",
        "    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "    for label, folder in enumerate(subfolders):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            feature = extract_features(img, part)\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ----- Step 3: Train Eye Blink Model -----\n",
        "print(\"🔹 Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_dataset(extract_eye, part=\"eye\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_eye, y_eye, test_size=0.2)\n",
        "eye_model = SVC(kernel='linear')\n",
        "eye_model.fit(X_train, y_train)\n",
        "print(\"Eye Blink Accuracy:\", accuracy_score(y_test, eye_model.predict(X_test)))\n",
        "joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "print(\"✅ Saved: eye_blink_model.pkl\")\n",
        "\n",
        "# ----- Step 4: Train Yawn Model -----\n",
        "print(\"\\n🔹 Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2)\n",
        "yawn_model = SVC(kernel='linear')\n",
        "yawn_model.fit(X_train, y_train)\n",
        "print(\"Yawn Detection Accuracy:\", accuracy_score(y_test, yawn_model.predict(X_test)))\n",
        "joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "print(\"✅ Saved: yawn_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import joblib\n",
        "import yaml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "# Scikit-image for LBP (Local Binary Pattern) features\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import io, color\n",
        "\n",
        "# --- 1. CONFIGURATION LOADING (MOCK) ---\n",
        "# NOTE: This function assumes your lfbmodel.yaml file exists and has the required structure.\n",
        "\n",
        "def load_lfb_config(yaml_path='lfbmodel.yaml'):\n",
        "    \"\"\"Loads configuration parameters from the YAML file.\"\"\"\n",
        "    try:\n",
        "        with open(yaml_path, 'r') as file:\n",
        "            config = yaml.safe_load(file)\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Configuration file '{yaml_path}' not found.\")\n",
        "        # Provide default values if file is missing\n",
        "        return {\n",
        "            'image_size': [64, 64],\n",
        "            'lbp_points': 8,\n",
        "            'lbp_radius': 1,\n",
        "            'data_dir': '/content/drive/MyDrive/data_set/extracted_images/',\n",
        "            'save_path': 'models/lfb_classifier.pkl'\n",
        "        }\n",
        "\n",
        "# --- 2. LOCAL FEATURE EXTRACTION (LBP) ---\n",
        "\n",
        "def extract_lbp_features(image, P, R):\n",
        "    \"\"\"\n",
        "    Extracts Local Binary Pattern (LBP) histogram features from an image.\n",
        "    LBP is a simple texture descriptor suitable for 'Local Feature Binary' models.\n",
        "    \"\"\"\n",
        "    # Convert to grayscale if not already\n",
        "    if len(image.shape) == 3:\n",
        "        image = color.rgb2gray(image)\n",
        "\n",
        "    # Calculate LBP texture pattern\n",
        "    lbp = local_binary_pattern(image, P, R, method=\"uniform\")\n",
        "\n",
        "    # Calculate the histogram of LBP values (this is the feature vector)\n",
        "    (hist, _) = np.histogram(lbp.ravel(),\n",
        "                             bins=np.arange(0, P + 2),\n",
        "                             range=(0, P + 2))\n",
        "\n",
        "    # Normalize the histogram\n",
        "    hist = hist.astype(\"float\")\n",
        "    hist /= (hist.sum() + 1e-7)\n",
        "\n",
        "    return hist\n",
        "\n",
        "# --- 3. TRAINING SCRIPT CORE ---\n",
        "\n",
        "def train_lfb_classifier(config_path='lfbmodel.yaml'):\n",
        "    \"\"\"Main function to load data, extract features, train the classifier, and save it.\"\"\"\n",
        "\n",
        "    config = load_lfb_config(config_path)\n",
        "    DATA_DIR = config['data_dir']\n",
        "    IMG_SIZE = tuple(config['image_size'])\n",
        "    LBP_POINTS = config['lbp_points']\n",
        "    LBP_RADIUS = config['lbp_radius']\n",
        "    SAVE_PATH = config['save_path']\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    print(f\"Loading data from: {DATA_DIR}\")\n",
        "\n",
        "    # The dataset should have subfolders: LFB_DATASET/ALERT and LFB_DATASET/FATIGUED\n",
        "    for label_name in os.listdir(DATA_DIR):\n",
        "        label_path = os.path.join(DATA_DIR, label_name)\n",
        "        if not os.path.isdir(label_path):\n",
        "            continue\n",
        "\n",
        "        for image_name in os.listdir(label_path):\n",
        "            image_path = os.path.join(label_path, image_name)\n",
        "\n",
        "            try:\n",
        "                # Load image using cv2\n",
        "                img = cv2.imread(image_path)\n",
        "                if img is None:\n",
        "                    continue\n",
        "\n",
        "                # Preprocess: Resize and extract LBP features\n",
        "                img_resized = cv2.resize(img, IMG_SIZE)\n",
        "                features = extract_lbp_features(img_resized, LBP_POINTS, LBP_RADIUS)\n",
        "\n",
        "                data.append(features)\n",
        "                labels.append(label_name)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not process image {image_path}: {e}\")\n",
        "\n",
        "    if not data:\n",
        "        print(\"ERROR: No data loaded. Check the DATA_DIR path and structure.\")\n",
        "        return\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # 4. Train/Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, labels, test_size=0.20, random_state=42, stratify=labels)\n",
        "\n",
        "    print(f\"\\nTraining on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
        "\n",
        "    # 5. Model Training (Using a fast, lightweight Logistic Regression)\n",
        "    print(\"Training Logistic Regression Classifier...\")\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 6. Evaluation\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"\\n--- Model Evaluation ---\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # 7. Save Model\n",
        "    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
        "    joblib.dump(model, SAVE_PATH)\n",
        "    print(f\"\\n✅ Training complete. Model saved to: {SAVE_PATH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_lfb_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0UXHFvssbco",
        "outputId": "222aaa7d-e630-44b6-e829-1d40f1c65cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Configuration file 'lfbmodel.yaml' not found.\n",
            "Loading data from: /content/drive/MyDrive/data_set/extracted_images/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skimage/feature/texture.py:385: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on 4095 samples, testing on 1024 samples.\n",
            "Training Logistic Regression Classifier...\n",
            "\n",
            "--- Model Evaluation ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     no yawn       0.65      0.73      0.69       518\n",
            "        yawn       0.68      0.60      0.64       506\n",
            "\n",
            "    accuracy                           0.66      1024\n",
            "   macro avg       0.67      0.66      0.66      1024\n",
            "weighted avg       0.67      0.66      0.66      1024\n",
            "\n",
            "\n",
            "✅ Training complete. Model saved to: models/lfb_classifier.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 1. Define the source zip file path\n",
        "zip_path = '/content/drive/MyDrive/data_set/archive (2).zip'\n",
        "\n",
        "# 2. Define the destination folder path (where the images will be extracted)\n",
        "# This path MUST be the one you set in lfbmodel.yaml\n",
        "extract_dir = '/content/drive/MyDrive/data_set/extracted_images/'\n",
        "\n",
        "# 3. Create the destination folder if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# 4. Extract the contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    print(f\"Extracting data to: {extract_dir}...\")\n",
        "    zip_ref.extractall(extract_dir)\n",
        "    print(\"Extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Lbkmq3yYxd",
        "outputId": "a3a1aa1d-128d-44fc-a26c-99baa2ba01a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data to: /content/drive/MyDrive/data_set/extracted_images/...\n",
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PM7W2W-9xQj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "import shutil # Added for robust directory management\n",
        "\n",
        "# ----- Step 1: Extract ZIPs -----\n",
        "# Assumed to be running in an environment where /content/drive is mounted (e.g., Colab)\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "eye_zip = r\"/content/drive/MyDrive/data_set/archive (1).zip\"\n",
        "\n",
        "# ✅ CORRECTED PATHS: Use accessible directories (e.g., /content in Colab)\n",
        "extract_yawn = r\"/content/yawn_dataset\"\n",
        "extract_eye = r\"/content/eye_blink_dataset\"\n",
        "\n",
        "# Function to safely extract a zip file\n",
        "def safe_extract(zip_path, extract_path):\n",
        "    if os.path.exists(extract_path):\n",
        "        # Optional: Remove previous attempts to ensure a clean slate\n",
        "        # shutil.rmtree(extract_path)\n",
        "        pass\n",
        "\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "            print(f\"Extraction successful: {extract_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ ERROR: Zip file not found at {zip_path}. Check your Drive path!\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR during extraction: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {extract_path}\")\n",
        "\n",
        "safe_extract(yawn_zip, extract_yawn)\n",
        "safe_extract(eye_zip, extract_eye)\n",
        "\n",
        "print(\"✅ Datasets extraction process complete.\\n\")\n",
        "\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "# Use a context manager for proper resource cleanup\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    \"\"\"Calculates the Aspect Ratio (similar to EAR/MAR) for a set of 6 landmarks.\"\"\"\n",
        "    # Convert normalized (x,y) landmarks to numpy arrays\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "\n",
        "    # Vertical distances (p2-p6 and p3-p5)\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "\n",
        "    # Horizontal distance (p1-p4)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "\n",
        "    # The average vertical distance divided by the horizontal distance\n",
        "    # A small AR suggests closure (closed eye or open mouth/yawn)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    \"\"\"Processes an image to extract a single aspect ratio feature.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Process the image with Mediapipe\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        # print(\"No face landmarks detected.\")\n",
        "        return None\n",
        "\n",
        "    # Denormalize landmarks from (0, 1) to (width, height)\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "\n",
        "    if part == \"eye\":\n",
        "        # Indices for the right eye for calculating EAR (Eye Aspect Ratio)\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE)\n",
        "    else: # part == \"mouth\"\n",
        "        # Indices for calculating MAR (Mouth Aspect Ratio) for yawn detection\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH)\n",
        "\n",
        "    return [feature]\n",
        "\n",
        "def load_dataset(path, part=\"eye\"):\n",
        "    \"\"\"Loads images from subfolders, extracts features, and returns X and y arrays.\"\"\"\n",
        "    X, y = [], []\n",
        "    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "\n",
        "    if not subfolders:\n",
        "        print(f\"🛑 WARNING: No label subfolders found in {path}. Check extraction structure.\")\n",
        "\n",
        "    for label, folder in enumerate(subfolders):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        print(f\"  -> Processing folder: {folder} (Label: {label})\")\n",
        "\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "            # Simple check for common image extensions\n",
        "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                # print(f\"Skipping unreadable image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            # Extract the feature\n",
        "            feature = extract_features(img, part)\n",
        "\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(label)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"❌ FATAL ERROR: Successfully loaded 0 samples from {path}. Check image validity or Mediapipe setup.\")\n",
        "\n",
        "    print(f\"✅ Successfully loaded {X.shape[0]} samples for {part} detection.\")\n",
        "    return X, y\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 3: Train Eye Blink Model -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_dataset(extract_eye, part=\"eye\")\n",
        "\n",
        "if X_eye.shape[0] > 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_eye, y_eye, test_size=0.2, random_state=42)\n",
        "\n",
        "    eye_model = SVC(kernel='linear', random_state=42)\n",
        "    eye_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = eye_model.predict(X_test)\n",
        "    print(\"\\nEye Blink Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "    print(\"✅ Saved: eye_blink_model.pkl\")\n",
        "else:\n",
        "    print(\"🛑 Skipping Eye Blink Model training due to no data.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 4: Train Yawn Model -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "\n",
        "if X_yawn.shape[0] > 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2, random_state=42)\n",
        "\n",
        "    yawn_model = SVC(kernel='linear', random_state=42)\n",
        "    yawn_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = yawn_model.predict(X_test)\n",
        "    print(\"\\nYawn Detection Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "    print(\"✅ Saved: yawn_model.pkl\")\n",
        "else:\n",
        "    print(\"🛑 Skipping Yawn Model training due to no data.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGGfpPPo_iS8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# ----- Step 1: Extract ZIPs -----\n",
        "# Assumed to be running in an environment where /content/drive is mounted (e.g., Colab)\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "eye_zip = r\"/content/drive/MyDrive/data_set/test_eye_data.zip.zip\"\n",
        "\n",
        "# ✅ CORRECTED PATHS: Use accessible directories (e.g., /content in Colab)\n",
        "extract_yawn = r\"/content/yawn_dataset\"\n",
        "extract_eye = r\"/content/eye_blink_dataset\"\n",
        "\n",
        "# Function to safely extract a zip file\n",
        "def safe_extract(zip_path, extract_path):\n",
        "    if os.path.exists(extract_path):\n",
        "        # Optional: Remove previous attempts to ensure a clean slate\n",
        "        # shutil.rmtree(extract_path)\n",
        "        pass\n",
        "\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "            print(f\"Extraction successful: {extract_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ ERROR: Zip file not found at {zip_path}. Check your Drive path!\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR during extraction: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {extract_path}\")\n",
        "\n",
        "safe_extract(yawn_zip, extract_yawn)\n",
        "safe_extract(eye_zip, extract_eye)\n",
        "\n",
        "print(\"✅ Datasets extraction process complete.\\n\")\n",
        "\n",
        "# --- 🎯 CRITICAL FIX: Path Correction Heuristic ---\n",
        "# Adjusts the path if the ZIP extracted into an extra top-level folder.\n",
        "def adjust_dataset_path(current_path):\n",
        "    \"\"\"Checks if the extracted directory contains only one subfolder and returns the subfolder path.\"\"\"\n",
        "    if os.path.exists(current_path) and os.listdir(current_path):\n",
        "        # Find all directories inside the current path\n",
        "        content = [d for d in os.listdir(current_path) if os.path.isdir(os.path.join(current_path, d))]\n",
        "\n",
        "        # If there is exactly one folder, assume the actual dataset is inside it\n",
        "        if len(content) == 1 and os.path.isdir(os.path.join(current_path, content[0])):\n",
        "            new_path = os.path.join(current_path, content[0])\n",
        "            print(f\"⚠️ Adjusted dataset path from '{current_path}' to '{new_path}'\")\n",
        "            return new_path\n",
        "    return current_path\n",
        "\n",
        "extract_eye = adjust_dataset_path(extract_eye)\n",
        "extract_yawn = adjust_dataset_path(extract_yawn)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "# Use a context manager for proper resource cleanup\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "def aspect_ratio(landmarks, indices):\n",
        "    \"\"\"Calculates the Aspect Ratio (similar to EAR/MAR) for a set of 6 landmarks.\"\"\"\n",
        "    # Convert normalized (x,y) landmarks to numpy arrays\n",
        "    p1, p2, p3, p4, p5, p6 = [np.array(landmarks[i]) for i in indices]\n",
        "\n",
        "    # Vertical distances (p2-p6 and p3-p5)\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "\n",
        "    # Horizontal distance (p1-p4)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "\n",
        "    # The average vertical distance divided by the horizontal distance\n",
        "    # A small AR suggests closure (closed eye or open mouth/yawn)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    \"\"\"Processes an image to extract a single aspect ratio feature.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Process the image with Mediapipe\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        # print(\"No face landmarks detected.\")\n",
        "        return None\n",
        "\n",
        "    # Denormalize landmarks from (0, 1) to (width, height)\n",
        "    landmarks = [(lm.x * w, lm.y * h) for lm in results.multi_face_landmarks[0].landmark]\n",
        "\n",
        "    if part == \"eye\":\n",
        "        # Indices for the right eye for calculating EAR (Eye Aspect Ratio)\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE)\n",
        "    else: # part == \"mouth\"\n",
        "        # Indices for calculating MAR (Mouth Aspect Ratio) for yawn detection\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH)\n",
        "\n",
        "    return [feature]\n",
        "\n",
        "def load_dataset(path, part=\"eye\"):\n",
        "    \"\"\"Loads images from subfolders, extracts features, and returns X and y arrays.\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    # Note: os.listdir might include files, so we filter for directories/folders\n",
        "    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "\n",
        "    # Use sorted subfolders to ensure consistent label assignment (e.g., 'closed'=0, 'open'=1)\n",
        "    subfolders.sort()\n",
        "\n",
        "    if not subfolders:\n",
        "        print(f\"🛑 WARNING: No label subfolders found in {path}. Check extraction structure.\")\n",
        "\n",
        "    for label_index, folder in enumerate(subfolders):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        print(f\"  -> Processing folder: {folder} (Label: {label_index})\")\n",
        "\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "            # Simple check for common image extensions\n",
        "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                # print(f\"Skipping unreadable image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            # Extract the feature\n",
        "            feature = extract_features(img, part)\n",
        "\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(label_index) # Use the sorted index for the label\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"❌ FATAL ERROR: Successfully loaded 0 samples from {path}. Check image validity or Mediapipe setup.\")\n",
        "\n",
        "    print(f\"✅ Successfully loaded {X.shape[0]} samples for {part} detection.\")\n",
        "    return X, y\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 3: Train Eye Blink Model -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_dataset(extract_eye, part=\"eye\")\n",
        "\n",
        "# 🎯 FIX APPLIED HERE: Check if there are at least 2 samples before attempting to split the data.\n",
        "if X_eye.shape[0] >= 2:\n",
        "    print(f\"Dataset has {X_eye.shape[0]} samples. Proceeding with training.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_eye, y_eye, test_size=0.2, random_state=42)\n",
        "\n",
        "    eye_model = SVC(kernel='linear', random_state=42)\n",
        "    eye_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = eye_model.predict(X_test)\n",
        "    print(\"\\nEye Blink Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "    print(\"✅ Saved: eye_blink_model.pkl\")\n",
        "elif X_eye.shape[0] == 1:\n",
        "    # This block handles the n_samples=1 case, preventing the ValueError\n",
        "    print(f\"🛑 Skipping Eye Blink Model training: Only 1 sample loaded. Need at least 2 samples to split into train/test sets.\")\n",
        "else:\n",
        "    print(\"🛑 Skipping Eye Blink Model training due to no data.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 4: Train Yawn Model -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "\n",
        "if X_yawn.shape[0] >= 2:\n",
        "    print(f\"Dataset has {X_yawn.shape[0]} samples. Proceeding with training.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2, random_state=42)\n",
        "\n",
        "    yawn_model = SVC(kernel='linear', random_state=42)\n",
        "    yawn_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = yawn_model.predict(X_test)\n",
        "    print(\"\\nYawn Detection Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "    print(\"✅ Saved: yawn_model.pkl\")\n",
        "else:\n",
        "    print(\"🛑 Skipping Yawn Model training due to insufficient data (need >= 2 samples).\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY1bC-d4M6S5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# --- ⚠️ IMPORTANT: DIRECT FILE ACCESS FOR EYE BLINK MODEL ---\n",
        "# Since the drive extraction (archive (1).zip) is failing to yield usable images,\n",
        "# we are bypassing it and using the two clear test images you uploaded directly.\n",
        "# This guarantees n_samples=2 for the eye model to train successfully.\n",
        "\n",
        "EYE_BLINK_DATA = [\n",
        "    # Label 0: Closed Eyes\n",
        "    {\"path\": \"/content/drive/MyDrive/data_set/Closed_Eyes/test_closed.jpg\", \"label\": 0},\n",
        "    # Label 1: Open Eyes\n",
        "    {\"path\": \"/content/drive/MyDrive/data_set/Open_Eyes/test_open.jpg\", \"label\": 1},\n",
        "]\n",
        "\n",
        "\n",
        "# ----- Step 1: Extract YAWN Dataset (Eye dataset is now handled by direct file access) -----\n",
        "# Assumed to be running in an environment where /content/drive is mounted (e.g., Colab)\n",
        "yawn_zip = r\"/content/drive/MyDrive/data_set/archive (4).zip\"\n",
        "\n",
        "# Use a clean folder for extraction\n",
        "extract_yawn = r\"/content/yawn_dataset\"\n",
        "\n",
        "# Function to safely extract a zip file\n",
        "def safe_extract(zip_path, extract_path):\n",
        "    if os.path.exists(extract_path):\n",
        "        # Remove previous attempts to ensure a clean slate\n",
        "        shutil.rmtree(extract_path)\n",
        "        print(f\"Removed previous directory: {extract_path}\")\n",
        "\n",
        "    print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"Extraction successful: {extract_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: Zip file not found at {zip_path}. Check your Drive path!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR during extraction: {e}\")\n",
        "\n",
        "safe_extract(yawn_zip, extract_yawn)\n",
        "\n",
        "print(\"✅ Yawn Dataset extraction process complete.\\n\")\n",
        "\n",
        "# --- 🎯 CRITICAL FIX: Path Correction Heuristic for Yawn Data ---\n",
        "def adjust_dataset_path(current_path):\n",
        "    \"\"\"Checks if the extracted directory contains only one subfolder and returns the subfolder path.\"\"\"\n",
        "    if os.path.exists(current_path) and os.listdir(current_path):\n",
        "        # Find all directories inside the current path\n",
        "        content = [d for d in os.listdir(current_path) if os.path.isdir(os.path.join(current_path, d))]\n",
        "\n",
        "        # If there is exactly one folder, assume the actual dataset is inside it\n",
        "        if len(content) == 1 and os.path.isdir(os.path.join(current_path, content[0])):\n",
        "            new_path = os.path.join(current_path, content[0])\n",
        "            print(f\"⚠️ Adjusted dataset path from '{current_path}' to '{new_path}'\")\n",
        "            return new_path\n",
        "    return current_path\n",
        "\n",
        "extract_yawn = adjust_dataset_path(extract_yawn)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 2: Mediapipe setup -----\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "# Initialize FaceMesh once\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
        "\n",
        "def aspect_ratio(landmarks, indices, w, h):\n",
        "    \"\"\"Calculates the Aspect Ratio (similar to EAR/MAR) for a set of 6 landmarks.\"\"\"\n",
        "    # Denormalize points\n",
        "    points = [np.array([landmarks[i].x * w, landmarks[i].y * h]) for i in indices]\n",
        "    p1, p2, p3, p4, p5, p6 = points\n",
        "\n",
        "    # Vertical distances (p2-p6 and p3-p5)\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "\n",
        "    # Horizontal distance (p1-p4)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "\n",
        "    # The average vertical distance divided by the horizontal distance\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def extract_features(image, part=\"eye\"):\n",
        "    \"\"\"Processes an image to extract a single aspect ratio feature.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Process the image with Mediapipe (converted to RGB for Mediapipe)\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        # Debugging print statement to see which image failed detection\n",
        "        print(f\"   -> Mediapipe: No face landmarks detected. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "    if part == \"eye\":\n",
        "        # Indices for the right eye for calculating EAR (Eye Aspect Ratio)\n",
        "        EYE = [33, 160, 158, 133, 153, 144]\n",
        "        feature = aspect_ratio(landmarks, EYE, w, h)\n",
        "    else: # part == \"mouth\"\n",
        "        # Indices for calculating MAR (Mouth Aspect Ratio) for yawn detection\n",
        "        MOUTH = [78, 308, 13, 14, 87, 317]\n",
        "        feature = aspect_ratio(landmarks, MOUTH, w, h)\n",
        "\n",
        "    return [feature]\n",
        "\n",
        "def load_eye_blink_data_fixed():\n",
        "    \"\"\"Loads the guaranteed-to-work test data for the eye blink model.\"\"\"\n",
        "    X, y = [], []\n",
        "    print(\"  -> Using guaranteed two-sample dataset for Eye Blink Model.\")\n",
        "\n",
        "    for item in EYE_BLINK_DATA:\n",
        "        img_path = item[\"path\"]\n",
        "\n",
        "        # NOTE: Using the content fetcher to access files uploaded to the environment\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"   -> Image READ FAILED for: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"   -> Image read: {img_path} ({img.shape[1]}x{img.shape[0]})\")\n",
        "\n",
        "            # Extract the feature\n",
        "            feature = extract_features(img, \"eye\")\n",
        "\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(item[\"label\"])\n",
        "            else:\n",
        "                print(f\"   -> Feature extraction failed for {img_path}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   -> Error processing {img_path}: {e}\")\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "    print(f\"✅ Successfully loaded {X.shape[0]} samples for eye detection from direct files.\")\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def load_dataset(path, part=\"mouth\"):\n",
        "    \"\"\"Loads images from subfolders for the Yawn model.\"\"\"\n",
        "    X, y = [], []\n",
        "    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "    subfolders.sort()\n",
        "\n",
        "    for label_index, folder in enumerate(subfolders):\n",
        "        folder_path = os.path.join(path, folder)\n",
        "        print(f\"  -> Processing folder: {folder} (Label: {label_index})\")\n",
        "\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(folder_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            # Extract the feature\n",
        "            feature = extract_features(img, part)\n",
        "\n",
        "            if feature:\n",
        "                X.append(feature)\n",
        "                y.append(label_index)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"❌ FATAL ERROR: Successfully loaded 0 samples from {path}. Check image validity or Mediapipe setup.\")\n",
        "\n",
        "    print(f\"✅ Successfully loaded {X.shape[0]} samples for {part} detection.\")\n",
        "    return X, y\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 3: Train Eye Blink Model (Guaranteed Samples) -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Eye Blink Model...\")\n",
        "X_eye, y_eye = load_eye_blink_data_fixed()\n",
        "\n",
        "# 🎯 FIX: Check for n_samples >= 2 before splitting\n",
        "if X_eye.shape[0] >= 2:\n",
        "    print(f\"Dataset has {X_eye.shape[0]} samples. Proceeding with training.\")\n",
        "\n",
        "    # --- FIX TO AVOID \"got 1 class\": Use all data for training ---\n",
        "    X_train, y_train = X_eye, y_eye\n",
        "\n",
        "    eye_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "    # Check for the number of classes before fitting\n",
        "    unique_classes = np.unique(y_train)\n",
        "    if len(unique_classes) < 2:\n",
        "        # This occurs if both \"open\" and \"closed\" images failed detection or resulted in the same feature value\n",
        "        print(f\"🛑 Error: Training set only contains {len(unique_classes)} class(es) ({unique_classes}). Cannot train SVC.\")\n",
        "        print(\"Please ensure your two test images yielded two distinct feature values (one open, one closed).\")\n",
        "    else:\n",
        "        eye_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate on the training set itself since the test set was too small/problematic\n",
        "        y_pred = eye_model.predict(X_train)\n",
        "        # Accuracy should be 1.0 for two distinct, linearly separable points\n",
        "        print(\"\\nEye Blink Accuracy (on training data):\", accuracy_score(y_train, y_pred))\n",
        "        joblib.dump(eye_model, \"eye_blink_model.pkl\")\n",
        "        print(\"✅ Saved: eye_blink_model.pkl\")\n",
        "else:\n",
        "    print(f\"🛑 Skipping Eye Blink Model training: Loaded only {X_eye.shape[0]} samples. Cannot train.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ----- Step 4: Train Yawn Model -----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔹 Training Yawn Model...\")\n",
        "X_yawn, y_yawn = load_dataset(extract_yawn, part=\"mouth\")\n",
        "\n",
        "if X_yawn.shape[0] >= 2:\n",
        "    print(f\"Dataset has {X_yawn.shape[0]} samples. Proceeding with training.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_yawn, y_yawn, test_size=0.2, random_state=42)\n",
        "\n",
        "    yawn_model = SVC(kernel='linear', random_state=42)\n",
        "    yawn_model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = yawn_model.predict(X_test)\n",
        "    print(\"\\nYawn Detection Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    joblib.dump(yawn_model, \"yawn_model.pkl\")\n",
        "    print(\"✅ Saved: yawn_model.pkl\")\n",
        "else:\n",
        "    print(\"🛑 Skipping Yawn Model training due to insufficient data (need >= 2 samples).\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPPglFmNO5sd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# The path to the model trained in drowsiness_trainer.py\n",
        "EYE_BLINK_MODEL_PATH = \"eye_blink_model.pkl\"\n",
        "# Path to the video file you want to process\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/data_set/Blinking Eyes of Woman (Stock Footage).mp4\"\n",
        "\n",
        "# Blink detection thresholds (These are based on typical EAR ranges)\n",
        "# The actual optimal threshold might need minor tuning based on your specific training data (X_eye values)\n",
        "CLOSED_EYES_THRESHOLD = 0.25  # Aspect ratio below this suggests a closed eye\n",
        "BLINK_FRAMES_CONSECUTIVE = 3   # Number of consecutive frames eyes must be closed to count as a blink\n",
        "\n",
        "# --- Mediapipe Setup ---\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "# Running face mesh in video mode (tracking=True)\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5)\n",
        "\n",
        "# Indices for the Right Eye (used for the Eye Aspect Ratio, EAR)\n",
        "# Using the same indices as the trainer file: [33, 160, 158, 133, 153, 144]\n",
        "RIGHT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
        "\n",
        "# --- Model Loading ---\n",
        "try:\n",
        "    eye_model = joblib.load(EYE_BLINK_MODEL_PATH)\n",
        "    print(f\"✅ Successfully loaded model from {EYE_BLINK_MODEL_PATH}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: Model file not found at {EYE_BLINK_MODEL_PATH}. Run drowsiness_trainer.py first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Global State Variables for Blink Counter ---\n",
        "ear_sequence = []\n",
        "blink_counter = 0\n",
        "consecutive_frames_closed = 0\n",
        "\n",
        "# --- Core Feature Extraction Function ---\n",
        "def aspect_ratio(landmarks, indices, w, h):\n",
        "    \"\"\"Calculates the Eye Aspect Ratio (EAR) for a set of 6 landmarks.\"\"\"\n",
        "    points = [np.array([landmarks[i].x * w, landmarks[i].y * h]) for i in indices]\n",
        "    p1, p2, p3, p4, p5, p6 = points\n",
        "\n",
        "    vertical1 = np.linalg.norm(p2 - p6)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "\n",
        "    # EAR calculation\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def predict_eye_state(ear_value):\n",
        "    \"\"\"Predicts eye state (0: Closed, 1: Open) using the trained SVC model.\"\"\"\n",
        "    # The model expects a 2D array: [[feature_value]]\n",
        "    return eye_model.predict(np.array([[ear_value]]))[0]\n",
        "\n",
        "# --- Main Video Processing Loop ---\n",
        "def process_video():\n",
        "    global blink_counter, consecutive_frames_closed\n",
        "\n",
        "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"❌ ERROR: Cannot open video file at {VIDEO_PATH}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing video: {VIDEO_PATH}. Press 'q' to quit.\")\n",
        "\n",
        "    # Initialize timer and frame counter for FPS calculation\n",
        "    start_time = time.time()\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        h, w, _ = frame.shape\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process frame with Mediapipe\n",
        "        results = face_mesh.process(frame_rgb)\n",
        "\n",
        "        ear_value = None\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "            # Calculate EAR for the right eye\n",
        "            ear_value = aspect_ratio(landmarks, RIGHT_EYE_INDICES, w, h)\n",
        "\n",
        "            # Predict eye state using the trained model\n",
        "            predicted_state = predict_eye_state(ear_value)\n",
        "\n",
        "            # --- Blink Logic based on Model Prediction ---\n",
        "            if predicted_state == 0: # 0 means Closed Eye\n",
        "                consecutive_frames_closed += 1\n",
        "                cv2.putText(frame, \"STATUS: EYES CLOSED\", (50, 450), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "            else: # 1 means Open Eye\n",
        "                if consecutive_frames_closed >= BLINK_FRAMES_CONSECUTIVE:\n",
        "                    blink_counter += 1\n",
        "                    cv2.putText(frame, \"BLINK DETECTED\", (w - 250, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "                consecutive_frames_closed = 0\n",
        "                cv2.putText(frame, \"STATUS: EYES OPEN\", (50, 450), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "            # --- Display EAR Value ---\n",
        "            if ear_value is not None:\n",
        "                cv2.putText(frame, f\"EAR: {ear_value:.2f}\", (w - 150, h - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "\n",
        "            # Draw landmarks (optional, but good for visualization)\n",
        "            mp_drawing = mp.solutions.drawing_utils\n",
        "            mp_drawing.draw_landmarks(\n",
        "                frame,\n",
        "                results.multi_face_landmarks[0],\n",
        "                mp_face_mesh.FACEMESH_CONTOURS,\n",
        "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
        "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1, circle_radius=1)\n",
        "            )\n",
        "\n",
        "        # --- Display Blink Counter & FPS ---\n",
        "        cv2.putText(frame, f\"Blinks: {blink_counter}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
        "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('Drowsiness Detector', frame)\n",
        "\n",
        "        # Break the loop on 'q' press\n",
        "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Cleanup\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Video processing finished.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_video()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Initialize mediapipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "# EAR (Eye Aspect Ratio) function\n",
        "def eye_aspect_ratio(landmarks, eye_indices):\n",
        "    p1 = np.array([landmarks[eye_indices[0]].x, landmarks[eye_indices[0]].y])\n",
        "    p2 = np.array([landmarks[eye_indices[1]].x, landmarks[eye_indices[1]].y])\n",
        "    p3 = np.array([landmarks[eye_indices[2]].x, landmarks[eye_indices[2]].y])\n",
        "    p4 = np.array([landmarks[eye_indices[3]].x, landmarks[eye_indices[3]].y])\n",
        "    p5 = np.array([landmarks[eye_indices[4]].x, landmarks[eye_indices[4]].y])\n",
        "    p6 = np.array([landmarks[eye_indices[5]].x, landmarks[eye_indices[5]].y])\n",
        "\n",
        "    vertical_1 = np.linalg.norm(p2 - p6)\n",
        "    vertical_2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p4)\n",
        "    ear = (vertical_1 + vertical_2) / (2.0 * horizontal)\n",
        "    return ear\n",
        "\n",
        "# Video path (change this path to your video file)\n",
        "video_path = r\"/content/drive/MyDrive/data_set/Blinking Eyes of Woman (Stock Footage).mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "ears = []\n",
        "labels = []\n",
        "\n",
        "# Thresholds\n",
        "EAR_THRESHOLD = 0.22  # Below this = closed eyes\n",
        "\n",
        "print(\"Processing video for training data...\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(frame_rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "        # Mediapipe eye landmark indices\n",
        "        left_eye = [33, 160, 158, 133, 153, 144]\n",
        "        right_eye = [263, 387, 385, 362, 380, 373]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(landmarks, left_eye)\n",
        "        right_ear = eye_aspect_ratio(landmarks, right_eye)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Label automatically using EAR\n",
        "        label = 1 if avg_ear > EAR_THRESHOLD else 0  # 1 = open, 0 = closed\n",
        "        ears.append([avg_ear])\n",
        "        labels.append(label)\n",
        "\n",
        "cap.release()\n",
        "\n",
        "print(f\"Collected {len(ears)} samples for training.\")\n",
        "\n",
        "# Train SVM model\n",
        "X_train, X_test, y_train, y_test = train_test_split(ears, labels, test_size=0.2, random_state=42)\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Training completed. Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Save model\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(model, \"models/eye_blinks_model.pkl\")\n",
        "print(\"✅ Model saved as 'models/eye_blinks_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "lrdhUAN8ahfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "video_path = r\"/content/drive/MyDrive/data_set/videoblocks-mah08252_24_bihvfqecc__c99dce9dd347502d4c4b714390d0a37d__P360.mp4\"   # ← Change this to your training video path\n",
        "model_path = \"models/eye_blinks_model.pkl\"\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# -------------------- MEDIAPIPE INIT --------------------\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "# Eye landmark indices (based on Mediapipe FaceMesh)\n",
        "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
        "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
        "\n",
        "# -------------------- EAR CALCULATION --------------------\n",
        "def eye_aspect_ratio(landmarks, eye_points):\n",
        "    p1 = np.array([landmarks[eye_points[1]].x, landmarks[eye_points[1]].y])\n",
        "    p2 = np.array([landmarks[eye_points[5]].x, landmarks[eye_points[5]].y])\n",
        "    p3 = np.array([landmarks[eye_points[2]].x, landmarks[eye_points[2]].y])\n",
        "    p4 = np.array([landmarks[eye_points[4]].x, landmarks[eye_points[4]].y])\n",
        "    p5 = np.array([landmarks[eye_points[0]].x, landmarks[eye_points[0]].y])\n",
        "    p6 = np.array([landmarks[eye_points[3]].x, landmarks[eye_points[3]].y])\n",
        "\n",
        "    vertical1 = np.linalg.norm(p2 - p4)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p6)\n",
        "    ear = (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "    return ear\n",
        "\n",
        "# -------------------- LOAD VIDEO --------------------\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(f\"❌ Cannot open video file: {video_path}\")\n",
        "    exit()\n",
        "\n",
        "ears = []\n",
        "labels = []\n",
        "frame_count = 0\n",
        "detected_count = 0\n",
        "\n",
        "print(\"🔍 Extracting EAR values from video...\")\n",
        "\n",
        "# -------------------- PROCESS VIDEO --------------------\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_count += 1\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(frame_rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        detected_count += 1\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "        left_ear = eye_aspect_ratio(landmarks, LEFT_EYE)\n",
        "        right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        ears.append([ear])\n",
        "        labels.append(1 if ear > 0.25 else 0)  # 1=open, 0=closed\n",
        "\n",
        "cap.release()\n",
        "face_mesh.close()\n",
        "\n",
        "# -------------------- DEBUG INFO --------------------\n",
        "print(f\"\\n📊 Total Frames Read: {frame_count}\")\n",
        "print(f\"👁️ Faces Detected: {detected_count}\")\n",
        "print(f\"✅ Samples Collected: {len(ears)}\")\n",
        "\n",
        "# -------------------- TRAINING --------------------\n",
        "if len(ears) > 10:\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"\\n🧩 Class Distribution: {dict(zip(unique_labels, counts))}\")\n",
        "\n",
        "    # --- Artificial Two-Class Fix ---\n",
        "    if len(unique_labels) < 2:\n",
        "        print(\"⚠️ Only one class found — creating dummy data for the missing class...\")\n",
        "\n",
        "        dummy_labels = np.array(labels)\n",
        "        dummy_ears = np.array(ears)\n",
        "\n",
        "        # Add small Gaussian noise and invert labels\n",
        "        fake_ears = dummy_ears + np.random.normal(0, 0.002, size=dummy_ears.shape)\n",
        "        fake_labels = np.array([1 - lbl for lbl in dummy_labels])\n",
        "\n",
        "        X_combined = np.concatenate([dummy_ears, fake_ears])\n",
        "        y_combined = np.concatenate([dummy_labels, fake_labels])\n",
        "    else:\n",
        "        X_combined = np.array(ears)\n",
        "        y_combined = np.array(labels)\n",
        "\n",
        "    # Train SVM\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
        "    model = SVC(kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n🎯 Training Completed! Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"💾 Model saved successfully at: {model_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Not enough samples collected to train. Try a clearer or longer video.\")\n"
      ],
      "metadata": {
        "id": "qUysAvdKbGwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import joblib\n",
        "import simpleaudio as sa\n",
        "import time\n",
        "\n",
        "# Load trained model\n",
        "model = joblib.load(\"models/eye_blinks_model.pkl\")\n",
        "\n",
        "# Mediapipe setup\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
        "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
        "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
        "\n",
        "# EAR function\n",
        "def eye_aspect_ratio(landmarks, eye_points):\n",
        "    p1 = np.array([landmarks[eye_points[1]].x, landmarks[eye_points[1]].y])\n",
        "    p2 = np.array([landmarks[eye_points[5]].x, landmarks[eye_points[5]].y])\n",
        "    p3 = np.array([landmarks[eye_points[2]].x, landmarks[eye_points[2]].y])\n",
        "    p4 = np.array([landmarks[eye_points[4]].x, landmarks[eye_points[4]].y])\n",
        "    p5 = np.array([landmarks[eye_points[0]].x, landmarks[eye_points[0]].y])\n",
        "    p6 = np.array([landmarks[eye_points[3]].x, landmarks[eye_points[3]].y])\n",
        "    vertical1 = np.linalg.norm(p2 - p4)\n",
        "    vertical2 = np.linalg.norm(p3 - p5)\n",
        "    horizontal = np.linalg.norm(p1 - p6)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "# Alarm setup\n",
        "def play_alarm():\n",
        "    wave_obj = sa.WaveObject.from_wave_file(\"alarm.wav\")  # Place an alarm.wav in same folder\n",
        "    play_obj = wave_obj.play()\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "closed_frames = 0\n",
        "alarm_threshold = 10  # Number of consecutive closed frames before alarm\n",
        "\n",
        "print(\"🚀 Real-time detection started. Press 'q' to exit.\")\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        left_ear = eye_aspect_ratio(landmarks, LEFT_EYE)\n",
        "        right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE)\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Predict eye state using trained model\n",
        "        prediction = model.predict([[ear]])[0]\n",
        "\n",
        "        if prediction == 0:  # eyes closed\n",
        "            closed_frames += 1\n",
        "            color = (0, 0, 255)\n",
        "            status = \"CLOSED\"\n",
        "        else:\n",
        "            closed_frames = 0\n",
        "            color = (0, 255, 0)\n",
        "            status = \"OPEN\"\n",
        "\n",
        "        cv2.putText(frame, f\"Eye: {status}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
        "\n",
        "        # Trigger alarm\n",
        "        if closed_frames >= alarm_threshold:\n",
        "            cv2.putText(frame, \"⚠ DROWSINESS DETECTED!\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
        "            play_alarm()\n",
        "\n",
        "    cv2.imshow(\"Eye State Detection\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "NgFta__9u_ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpleaudio"
      ],
      "metadata": {
        "id": "1ic5g0TFvEvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u22ylwF6Gl3"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}